# Research
paper &amp; learning tutorial reading list
## Quick Link
- [Topic - Dialogue System](https://github.com/TroisLiu/research/blob/master/README.md#dialogue-system)
- [Topic - LLM](https://github.com/TroisLiu/research/tree/master#llm)
- [Topic - æ¨è–¦ç³»çµ±](https://github.com/TroisLiu/research/tree/master#recommendation-system)
- [Topic - Knowledge](https://github.com/TroisLiu/research/tree/master#knowledge)

## Dialogue System
- å°è©±ç‹€æ…‹è¿½è¹¤(Dialogue State Tracking, DST)
  - LLMè¼ƒå–„æ–¼åˆ©ç”¨å®Œæ•´å°è©±èªå¢ƒä¾†ç³¾æ­£å…ˆå‰è¼ªæ¬¡å¯èƒ½å‡ºç¾çš„éŒ¯èª¤
  - LLM-DSTçš„æ•´é«”æ€§èƒ½éš¨å°è©±è¼ªæ•¸å¢åŠ è€Œä¸‹é™å¾—æ›´æ…¢ï¼Œå°éŒ¯èª¤å‚³æ’­ï¼ˆerror propagationï¼‰çš„æŠµæŠ—åŠ›æ¯”å‚³çµ±æ¨¡å‹æ›´ä½³
  - è«–æ–‡ï¼š
    - [(2023)Towards LLM-driven Dialogue State Tracking](https://aclanthology.org/2023.emnlp-main.48.pdf)
      - æå‡ºLDSTæ¡†æ¶: åŸºæ–¼é–‹æºLLaMAæ¨¡å‹é€éæŒ‡ä»¤å¾®èª¿(domain-slot instruction tuning)å¯¦ç¾
    - [(2024)Chain of Thought Explanation for Dialogue State Tracking](https://arxiv.org/html/2403.04656v1)
      - å°‡CoTå¼•å…¥è‡³DSTç•¶ä¸­ï¼Œæ¨¡å‹åœ¨æ±ºå®šæ§½ä½å€¼å¾Œç”Ÿæˆé€æ­¥æ¨ç†çš„è§£é‡‹
      - èƒ½å¼•å°æ¨¡å‹å¾ç›¸é—œå°è©±è¼ªä¸­è’é›†è³‡è¨Šä¸¦æ¨ç†æ­£ç¢ºçš„æ§½å€¼ï¼Œå¾è€Œæé«˜é æ¸¬çš„æº–ç¢ºå¯é æ€§
    - [(2024)Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation](https://aclanthology.org/2024.acl-long.473.pdf)
      - å› äººå·¥æ¨™è¨»æˆæœ¬é«˜ï¼Œåˆ©ç”¨LLMä¾†æ¨¡æ“¬å°è©±ç”Ÿæˆæ¨™è¨»æ•¸æ“š
      - è«–æ–‡ä½¿ç”¨GPT-4å……ç•¶ç”¨æˆ¶å’Œä»£ç†äººï¼Œç”Ÿæˆå¸¶æœ‰DSTæ¨™ç±¤çš„å¤§é‡æ¨¡æ“¬å°è©±ï¼Œç„¶å¾Œç”¨é€™äº›åˆæˆæ•¸æ“šå°LLaMAÂ 2æ¨¡å‹é€²è¡Œå…©éšæ®µå¾®èª¿ï¼Œåœ¨MultiWOZç­‰è³‡æ–™é›†ä¸Šå–å¾—å„ªæ–¼åƒ…ç”¨çœŸå¯¦æ•¸æ“šè¨“ç·´çš„æ•ˆæœ
## Knowledge 
- Knowledge Augmentation
- Knowledge Extraction
- Knowledge Graph
- Knowledge Fusion
- Knowledge Inference
- Knowledge Update
- Knowledge Validation

## LLM
### prompt engineering
####  åŸå‰‡ 1ï¼šæ’°å¯«æ¸…æ™°ä¸”å…·é«”çš„æŒ‡ç¤º
  - ç­–ç•¥ 1ï¼šä½¿ç”¨åˆ†éš”ç¬¦æ¸…æ¥šåœ°æ¨™ç¤ºè¼¸å…¥ä¸­ä¸åŒçš„éƒ¨åˆ†
    ```
    prompt = f"""
    Summarize the text delimited by triple backticks \ 
    into a single sentence.
    ```{text}```
    """
    ```
  - ç­–ç•¥ 2ï¼šè¦æ±‚æä¾›çµæ§‹åŒ–çš„è¼¸å‡º
    ```
    prompt = f"""
    Generate a list of three made-up book titles along \ 
    with their authors and genres. 
    Provide them in JSON format with the following keys: 
    book_id, title, author, genre.
    """
    ```    
  - ç­–ç•¥ 3ï¼šè¦æ±‚æ¨¡å‹æª¢æŸ¥æ¢ä»¶æ˜¯å¦æ»¿è¶³
    ```
    prompt = f"""
    You will be provided with text delimited by triple quotes. 
    If it contains a sequence of instructions, \ 
    re-write those instructions in the following format:
    
    Step 1 - ...
    Step 2 - â€¦
    â€¦
    Step N - â€¦
    
    If the text does not contain a sequence of instructions, \ 
    then simply write \"No steps provided.\"
    
    \"\"\"{text_1}\"\"\"
    """
    ```        
  - ç­–ç•¥ 4ï¼šFew-shot Promptingã€Œå°‘é‡ç¯„ä¾‹ã€æç¤º
    ```
    prompt = f"""
    You will be provided with text delimited by triple quotes. 
    If it contains a sequence of instructions, \ 
    re-write those instructions in the following format:
    
    Step 1 - ...
    Step 2 - â€¦
    â€¦
    Step N - â€¦
    
    If the text does not contain a sequence of instructions, \ 
    then simply write \"No steps provided.\"
    
    \"\"\"{text_1}\"\"\"
    """
    ```         
#### åŸå‰‡ 2ï¼šçµ¦æ¨¡å‹æ™‚é–“é€²è¡Œã€Œæ€è€ƒã€
  - ç­–ç•¥ 1ï¼šæ˜ç¢ºæŒ‡å‡ºå®Œæˆä»»å‹™æ‰€éœ€çš„æ­¥é©Ÿ
    ```
    f"""
    Your task is to perform the following actions: 
    1 - Summarize the following text delimited by 
      <> with 1 sentence.
    2 - Translate the summary into French.
    3 - List each name in the French summary.
    4 - Output a json object that contains the 
      following keys: french_summary, num_names.
    
    Use the following format:
    Text: <text to summarize>
    Summary: <summary>
    Translation: <summary translation>
    Names: <list of names in summary>
    Output JSON: <json with summary and num_names>
    
    Text: <{text}>
    """
    ```
  - ç­–ç•¥ 2ï¼šæŒ‡å°æ¨¡å‹åœ¨å¾—å‡ºçµè«–ä¹‹å‰å…ˆè‡ªè¡Œæ¨å°è§£æ±ºæ–¹æ¡ˆ
    ```
    prompt = f"""
    Your task is to determine if the student's solution \
    is correct or not.
    To solve the problem do the following:
    - First, work out your own solution to the problem including the final total. 
    - Then compare your solution to the student's solution \ 
    and evaluate if the student's solution is correct or not. 
    Don't decide if the student's solution is correct until 
    you have done the problem yourself.
    
    Use the following format:
    Question:
    '''
    question here
    '''
    Student's solution:
    '''
    student's solution here
    '''
    Actual solution:
    '''
    steps to work out the solution and your solution here
    '''
    Is the student's solution the same as actual solution \
    just calculated:
    '''
    yes or no
    '''
    Student grade:
    '''
    correct or incorrect
    '''
    
    Question:
    '''
    I'm building a solar power installation and I need help \
    working out the financials. 
    - Land costs $100 / square foot
    - I can buy solar panels for $250 / square foot
    - I negotiated a contract for maintenance that will cost \
    me a flat $100k per year, and an additional $10 / square \
    foot
    What is the total cost for the first year of operations \
    as a function of the number of square feet.
    '''
    Student's solution:
    '''
    Let x be the size of the installation in square feet.
    Costs:
    1. Land cost: 100x
    2. Solar panel cost: 250x
    3. Maintenance cost: 100,000 + 100x
    Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
    '''
    Actual solution:
    """
    ```
#### å…¶ä»–Task
- Summary
  ```
      prompt = f"""
    Your task is to generate a short summary of a product \ 
    review from an ecommerce site. 

    Summarize the review below, delimited by triple \
    backticks in at most 20 words. 

    Review: ```{reviews[i]}```
    """
  ```
- Sentiment (Positive/Negative)
  ```
  prompt = f"""
  What is the sentiment of the following product review, 
  which is delimited with triple backticks?

  Give your answer as a single word, either "positive" \
  or "negative".
  
  Review text: '''{lamp_review}'''
  """
  ```
- Emotion
  ```
  prompt = f"""
  Identify a list of emotions that the writer of the \
  following review is expressing. Include no more than \
  five items in the list. Format your answer as a list of \
  lower-case words separated by commas.
  
  Review text: '''{lamp_review}'''
  """

  prompt = f"""
  Is the writer of the following review expressing anger?\
  The review is delimited with triple backticks. \
  Give your answer as either yes or no.
  
  Review text: '''{lamp_review}'''
  """
  ```
- Extract Entity
  ```
  prompt = f"""
  Identify the following items from the review text: 
  - Item purchased by reviewer
  - Company that made the item
  
  The review is delimited with triple backticks. \
  Format your response as a JSON object with \
  "Item" and "Brand" as the keys. 
  If the information isn't present, use "unknown" \
  as the value.
  Make your response as short as possible.
    
  Review text: '''{lamp_review}'''
  """
  ```
- Multiple Task at once
  ```
  prompt = f"""
  Identify the following items from the review text: 
  - Sentiment (positive or negative)
  - Is the reviewer expressing anger? (true or false)
  - Item purchased by reviewer
  - Company that made the item
  
  The review is delimited with triple backticks. \
  Format your response as a JSON object with \
  "Sentiment", "Anger", "Item" and "Brand" as the keys.
  If the information isn't present, use "unknown" \
  as the value.
  Make your response as short as possible.
  Format the Anger value as a boolean.
  
  Review text: '''{lamp_review}'''
  """
  ```
- Translation
  ```
  prompt = f"""
  Translate the following English text to Spanish: \ 
  '''Hi, I would like to order a blender'''
  """

  prompt = f"""
  Tell me which language this is: 
  '''Combien coÃ»te le lampadaire?'''
  """

  prompt = f"""
  Translate the following  text to French and Spanish
  and English pirate: \
  '''I want to order a basketball'''
  """

  prompt = f"""
  Translate the following text to Spanish in both the \
  formal and informal forms: 
  'Would you like to order a pillow?'
  """
  ```
- Writing Style Transformation
  ```
  prompt = f"""
  Translate the following from slang to a business letter: 
  'Dude, This is Joe, check out this spec on this standing lamp.'
  """
  ```
- Writing Format Conversion
  ```
  prompt = f"""
  Translate the following python dictionary from JSON to an HTML \
  table with column headers and title: {data_json}
  """
  ```
- Spell/Grammer Check
  ```
    prompt = f"""Proofread and correct the following text
    and rewrite the corrected version. If you don't find
    and errors, just say "No errors found". Don't use 
    any punctuation around the text:
    '''{t}'''"""
  ```
- prompt in messages data of api parameters
  ```
  messages.append(
  {'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\
   The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size        5)total price '},    
  )
  ```
### æ©Ÿåˆ¶
#### Chain of Thought

#### Tree of Thought
- [(2023)Tree of Thoughts: Deliberate Problem Solving with Large Language Models.md](https://github.com/TroisLiu/research/blob/master/Tree%20of%20Thoughts%3A%20Deliberate%20Problem%20Solving%20with%20Large%20Language%20Models.md)

#### è¯é‚¦å­¸ç¿’
##### reference
- [æ€è€ƒä¸€ä¸‹ï¼Œè”é‚¦å­¦ä¹ å¯ä»¥è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹å—ï¼Ÿ](https://www.jiqizhixin.com/articles/2023-07-10-4)

#### Machnine Unlearning
##### reference
- [ç»™æœºå™¨ä¸‹ã€Œé—å¿˜å’’ã€ï¼Ÿè°·æ­Œå‘èµ·é¦–ä¸ªæœºå™¨é—å¿˜æŒ‘æˆ˜èµ›](https://www.jiqizhixin.com/articles/2023-07-09-4)

### AI Agent
#### Reference
- [GPT-4o æ–°çªç ´ï¼AI åƒ…éœ€å…©å°æ™‚è¨ªè«‡ï¼Œè¼•é¬†è¤‡è£½äººé¡æ€§æ ¼ï¼Œæº–ç¢ºç‡é«˜é” 85%](https://www.techbang.com/posts/120060-gpt-4o-replicates-human-personality?fbclid=IwY2xjawHNr2pleHRuA2FlbQIxMQABHX4uWXcoR5-M7J8WEzKtHcX0408vX2lHKnAV0W8SjLAxVNZSOiPV5d_xEQ_aem_63DQ3T9dMgtkd2p9YP_ZEg)
- [è¿‘æœŸæ­£å¤¯çš„AI Agentï¼ˆæ™ºæ…§å‹ä»£ç†ï¼‰åˆ°åº•æ˜¯ä»€éº¼ï¼Ÿ](https://www.ectimes.org.tw/2024/05/%E7%88%86%E7%B4%85%E7%9A%84ai-agent%EF%BC%88%E6%99%BA%E6%85%A7%E5%9E%8B%E4%BB%A3%E7%90%86%EF%BC%89%EF%BC%8C%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E9%BA%BC%EF%BC%9F/)
- [AI Agentæ¦‚å¿µåŠå…¶æ‡‰ç”¨](https://medium.com/vincent-chen/ai-agent%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%85%B6%E6%87%89%E7%94%A8-e66c88e9a015)
- [å®Œæ•´æŒ‡å—LLM 2024å¹´ä»£ç†](https://botpress.com/tw/blog/llm-agents)

### å¾®èª¿
#### Adapter
- ä»‹ç´¹
- å„ªé»
  - å‚æ•°æ•ˆç‡æ›´é«˜ï¼šä¸€ä¸ªä»»åŠ¡åªéœ€è¦å°‘é‡å‚æ•°ï¼Œè®­ç»ƒæ›´å¿«ï¼Œå ç”¨çš„å†…å­˜æ›´å°‘ï¼Œå¯¹æ•°æ®é›†è¾ƒå°çš„ä»»åŠ¡æ›´éš¾è¿‡æ‹Ÿåˆï¼Œä¹Ÿæ›´æœ‰åˆ©äºæ¨¡å‹çš„å­˜å‚¨å’Œåˆ†å‘
  - è¿ç»­å­¦ä¹ çš„é—å¿˜é—®é¢˜ï¼šAdapter å†»ç»“äº†åŸæœ‰æ¨¡å‹çš„å‚æ•°ï¼Œä¿è¯äº†åŸæ¥çš„çŸ¥è¯†ä¸è¢«é—å¿˜ã€‚
  - å¤šä»»åŠ¡å­¦ä¹ ï¼šä½¿ç”¨ adapter ä¹Ÿå¯ä»¥ç”¨æ¯”è¾ƒå°‘é‡çš„å‚æ•°å­¦ä¹ å¤šä¸ªä»»åŠ¡
#### LoRA (Low-Rank Adaptation of LLM)
- PEFT(Parameter-Efficient Fine-Tuning)çš„æŠ€è¡“ä¹‹ä¸€
- ä»‹ç´¹ï¼šåœ¨åŸæ¨¡å‹åŸºç¤ä¸Šï¼Œå¢åŠ Adapteræ–¼å¦ä¸€åˆ†æ”¯è·¯å¾‘ï¼Œè¨“ç·´æ™‚å¯æ“‡ä¸€è·¯å¾‘è¨“ç·´
  - å¾®èª¿æ™‚ï¼Œèµ°Adapterè·¯å¾‘ï¼Œåªæ›´æ–°Adapterçš„åƒæ•¸ï¼Œä¿æŒåŸPLMåƒæ•¸ä¸ä¾¿
  - æ¨è«–æ™‚ï¼Œèµ°åŸè·¯å¾‘ï¼Œä»£è¡¨åªä½¿ç”¨åŸæ¨¡å‹åƒæ•¸ï¼Œæ²’æœ‰éºå¿˜åŸæœ¬çŸ¥è­˜
  - æ¨è«–æ™‚ï¼Œèµ°Adapterè·¯å¾‘ï¼Œèµ°å¾®èª¿çŸ¥è­˜ï¼Œä¸èµ°åŸPLMçŸ¥è­˜
- å„ªé»
  - ä½ç§©åˆ†è§£ï¼šå¾å–®ä¸€å¤§çŸ©é™£W(d*d) è®Šæˆå…©å€‹å°çŸ©é™£U (d*r)èˆ‡V (r*d)çš„ä¹˜ç©ï¼Œé …ä½è¨ˆç®—è¤‡é›œåº¦
    - å°† LoRA çš„ç§©rè®¾ç½®ä¸ºé¢„è®­ç»ƒæƒé‡çŸ©é˜µçš„ç§©ï¼Œå°±èƒ½å¤§è‡´æ¢å¤äº†å…¨é‡å¾®è°ƒçš„è¡¨ç°åŠ›ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œéšç€å¢åŠ å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œè®­ç»ƒ LoRA å¤§è‡´æ”¶æ•›äºè®­ç»ƒåŸå§‹æ¨¡å‹ã€‚
  - å¤šç²’åº¦ï¼šå¯ä»¥é‡å°è©ã€æ®µè½ã€Downstream-taskå¤šç¨®é¡†ç²’åº¦å»å»ºæ¨¡
  - ç•°æ§‹æ¡†æ¶ï¼šå¯æ‡‰ç”¨æ–¼ä¸åŒçµæ§‹çš„transformerç•¶ä¸­
  - ç„¡ç›£ç£å­¸ç¿’ï¼šä½¿ç”¨é æ¸¬-èšåˆæå¤±å‡½æ•¸é€²è¡Œadapterçš„åƒæ•¸å­¸ç¿’ï¼Œç„¡é ˆå¦å¤–æ¨™è¨»è³‡æ–™
  - æ²¡æœ‰é¢å¤–çš„æ¨ç†å»¶æ—¶
  - å‡å°‘å†…å­˜å’Œå­˜å‚¨èµ„æºæ¶ˆè€—
- ç—›é»
  - åƒæ•¸ç©ºé–“å°ï¼šLoRAä¸­å‚ä¸è®­ç»ƒçš„å‚æ•°é‡è¾ƒå°‘ï¼Œè§£ç©ºé—´è¾ƒå°ï¼Œæ•ˆæœç›¸æ¯”å…¨é‡å¾®è°ƒæœ‰ä¸€å®šçš„å·®è·ã€‚
  - å¾®è°ƒå¤§æ¨¡å‹æˆæœ¬é«˜ï¼šå¯¹äºä¸Šç™¾äº¿å‚æ•°é‡çš„æ¨¡å‹ï¼ŒLoRAå¾®è°ƒçš„æˆæœ¬è¿˜æ˜¯å¾ˆé«˜ã€‚
  - ç²¾åº¦æŸå¤±ï¼šé’ˆå¯¹ç¬¬äºŒç‚¹ï¼Œå¯ä»¥é‡‡ç”¨int8æˆ–int4é‡åŒ–ï¼Œè¿›ä¸€æ­¥å¯¹æ¨¡å‹åŸºåº§çš„å‚æ•°è¿›è¡Œå‹ç¼©ã€‚ä½†æ˜¯åˆä¼šå¼•å‘ç²¾åº¦æŸå¤±çš„é—®é¢˜ï¼Œé™ä½æ¨¡å‹æ€§èƒ½ã€‚
- èˆ‡ä¼ ç»Ÿçš„å¤šä»»åŠ¡å­¦ä¹ ç›¸æ¯”
  - å¥½å¤„ï¼šä¸åŒä»»åŠ¡ä¹‹é—´å½±å“è¾ƒå°‘
  - åå¤„ï¼šä¸åŒä»»åŠ¡å¸¦æ¥çš„ç›¸äº’çš„ç›‘ç£å¯èƒ½ä¼šå˜å°‘ 
##### reference
- [è¯¦è§£å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•LoRA Adapter(å†…é™„å®ç°ä»£ç )](https://mltalks.medium.com/%E8%AF%A6%E8%A7%A3%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95lora-adapter-%E5%86%85%E9%99%84%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81-aff4fb42beec)
- [ICLR2022é«˜åˆ†æ–‡ç« ï¼šå°†Adapterã€prompt-tuningã€LoRAç­‰çº³å…¥ç»Ÿä¸€çš„æ¡†æ¶](https://zhuanlan.zhihu.com/p/436571527)
- [å¾®èª¿å¤§å‹èªè¨€æ¨¡å‹LLMçš„æŠ€è¡“LoRAåŠç”Ÿæˆå¼AI-Stable diffusion LoRA](https://xiaosean5408.medium.com/%E5%BE%AE%E8%AA%BF%E5%A4%A7%E5%9E%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8Bllm%E7%9A%84%E6%8A%80%E8%A1%93lora%E5%8F%8A%E7%94%9F%E6%88%90%E5%BC%8Fai-stable-diffusion-lora-61a41d636772)
- [ã€peftã€‘huggingfaceå¤§æ¨¡å‹åŠ è½½å¤šä¸ªLoRAå¹¶éšæ—¶åˆ‡æ¢](https://blog.csdn.net/liuqixuan1994/article/details/130664198)
  - ä»‹ç´¹å¦‚ä½•ä½¿ç”¨ Huggingface - PEFT
- [LLM - LoRA æ¨¡å‹åˆå¹¶ä¸ä¿å­˜](https://bitddd.blog.csdn.net/article/details/132065177)
  - LoRAé€émerge_and_unload() ï¼šå¯å°‡adapterèˆ‡åŸºæœ¬æ¨¡å‹åˆä½µè¼‰å…¥ï¼Œæ¶ˆé™¤å»¶é²
  - Codeå¯¦ä½œä»‹ç´¹
- [Github - Huggingface - PEFT](https://github.com/huggingface/peft)
  - é–‹æºæ¡†æ¶PEFT
    - LoRA:
      - é€šè¿‡å­¦ä¹ å°å‚æ•°çš„ä½ç§©çŸ©é˜µæ¥è¿‘ä¼¼æ¨¡å‹æƒé‡çŸ©é˜µWçš„å‚æ•°æ›´æ–°ï¼Œè®­ç»ƒæ—¶åªä¼˜åŒ–ä½ç§©çŸ©é˜µå‚æ•°ã€‚
      - åƒè€ƒå‰é¢èªªæ˜ 
    - P-Tuning:    
      - åˆ©ç”¨å°‘é‡è¿ç»­çš„ embedding å‚æ•°ä½œä¸º prompt ä½¿ GPT æ›´å¥½çš„åº”ç”¨äº NLU ä»»åŠ¡
      - P-Tuning åªåœ¨ embedding å±‚å¢åŠ å‚æ•°
      - P-Tuning V2æ–¹æ³•çš„æ€è·¯å…¶å®å’Œ Prefix-Tuning ç›¸ä¼¼ï¼Œåœ¨æ¨¡å‹çš„æ¯ä¸€å±‚éƒ½åº”ç”¨è¿ç»­çš„ prompts å¹¶å¯¹ prompts å‚æ•°è¿›è¡Œæ›´æ–°ä¼˜åŒ–ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•æ˜¯é’ˆå¯¹ NLU ä»»åŠ¡ä¼˜åŒ–å’Œé€‚é…çš„ã€‚
    - Prefix Tuning:
      - Prefix-Tuning æ˜¯é’ˆå¯¹ NLG ä»»åŠ¡è®¾è®¡
      - å›ºå®š PLM çš„æ‰€æœ‰å‚æ•°ï¼Œåªæ›´æ–°ä¼˜åŒ–ç‰¹å®šä»»åŠ¡çš„ prefix
      - Prefix-Tuning åœ¨æ¯ä¸€å±‚éƒ½æ·»åŠ å¯è®­ç»ƒå‚æ•°
      - åœ¨æ¨¡å‹è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªè¿ç»­çš„ä¸”ä»»åŠ¡ç‰¹å®šçš„å‘é‡åºåˆ—ï¼ˆcontinuous task-specific vectorsï¼‰ï¼Œç§°ä¹‹ä¸ºå‰ç¼€ï¼ˆprefixï¼‰ã€‚å‰ç¼€è¢«è§†ä¸ºä¸€ç³»åˆ—â€œè™šæ‹Ÿ tokensâ€
      - å„ªé»
        - åªéœ€è¦ä¸ºæ¯ä¸ªä»»åŠ¡å­˜å‚¨ç‰¹å®š prefixï¼Œä½¿ Prefix-tuning æ¨¡å—åŒ–ä¸”èŠ‚çœå­˜å‚¨ç©ºé—´
        - Prefix-tuning æ€§èƒ½å¥½äº Infix-tuningï¼Œå› ä¸º prefix èƒ½å¤ŸåŒæ—¶å½±å“xå’Œyçš„éšå±‚æ¿€æ´»ï¼Œè€Œ infix åªèƒ½å¤Ÿå½±å“yçš„éšå±‚æ¿€æ´»
      - ç¼ºé»
        - Prefix Tuning éš¾ä»¥ä¼˜åŒ–ï¼Œå…¶æ€§èƒ½éšå¯è®­ç»ƒå‚æ•°è§„æ¨¡éå•è°ƒå˜åŒ–ï¼Œ
        - ä¸ºå‰ç¼€ä¿ç•™éƒ¨åˆ†åºåˆ—é•¿åº¦å¿…ç„¶ä¼šå‡å°‘ç”¨äºå¤„ç†ä¸‹æ¸¸ä»»åŠ¡çš„åºåˆ—é•¿åº¦ã€‚ 
    - Prompt Tuning:
      - å¯ä»¥çœ‹åšæ˜¯ Prefix Tuning çš„ç®€åŒ–
      - å›ºå®šæ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œåªå…è®¸å°†æ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡çš„é¢å¤–kä¸ªå¯æ›´æ–°çš„ tokens å‰ç½®åˆ°è¾“å…¥æ–‡æœ¬ä¸­ï¼Œä¹Ÿæ²¡æœ‰ä½¿ç”¨é¢å¤–çš„ç¼–ç å±‚æˆ–ä»»åŠ¡ç‰¹å®šçš„è¾“å‡ºå±‚ã€‚
      - åœ¨æ¨¡å‹çš„è¾“å…¥æˆ–éšå±‚æ·»åŠ Kä¸ªé¢å¤–å¯è®­ç»ƒçš„å‰ç¼€ tokensï¼ˆè¿™äº›å‰ç¼€æ˜¯è¿ç»­çš„ä¼ª tokensï¼Œä¸å¯¹åº”çœŸå®çš„ tokensï¼‰ï¼Œåªè®­ç»ƒè¿™äº›å‰ç¼€å‚æ•°ï¼›
      - æå‡ºäº† Prompt Ensembling æ–¹æ³•æ¥é›†æˆé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å¤šç§ prompts
        - åœ¨åŒä¸€ä»»åŠ¡ä¸Šè®­ç»ƒnä¸ªpromptsï¼Œä¸ºä¸€ä¸ªä»»åŠ¡åˆ›å»ºäº†nä¸ªå•ç‹¬çš„æ¨¡å‹ï¼ŒåŒæ—¶åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å…±äº«æ ¸å¿ƒçš„é¢„è®­ç»ƒè¯­è¨€å»ºæ¨¡å‚æ•°
    - Adapter-Tuning (PEFTæŠ€è¡“ä¹‹ä¸€ï¼Œå¦‚LLM-Adaptersæ˜¯å…¶å¯¦ä½œ)
      - å‰å…©å€‹æ˜¯æ·»åŠ prompt embedding å‚æ•°æ¥ä»¥å°‘é‡å‚æ•°é€‚é…ä¸‹æ¸¸ä»»åŠ¡
      - å°†è¾ƒå°çš„ç¥ç»ç½‘ç»œå±‚æˆ–æ¨¡å—æ’å…¥é¢„è®­ç»ƒæ¨¡å‹çš„æ¯ä¸€å±‚ï¼Œè¿™äº›æ–°æ’å…¥çš„ç¥ç»æ¨¡å—ç§°ä¸º adapterï¼ˆé€‚é…å™¨ï¼‰ï¼Œä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ—¶ä¹Ÿåªè®­ç»ƒè¿™äº›é€‚é…å™¨å‚æ•°
      - ç¨®é¡ï¼šä¸»è¦åŒ…æ‹¬ Series Adapterï¼ˆä¸²è¡Œï¼‰ å’Œ Parallel Adapterï¼ˆå¹¶è¡Œï¼‰
      - ç¼ºé» 
        - Adapter Tuning åœ¨ PLM åŸºç¡€ä¸Šæ·»åŠ é€‚é…å™¨å±‚ä¼šå¼•å…¥é¢å¤–çš„è®¡ç®—ï¼Œå¸¦æ¥æ¨ç†å»¶è¿Ÿé—®é¢˜
- [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2304.01933)
  - LLM-Adapteræ˜¯å¯¹ PEFT åº“çš„æ‰©å±•ï¼Œæ˜¯ä¸€ä¸ªç®€å•æ˜“ç”¨çš„æ¡†æ¶ï¼Œå°†å„ç§é€‚é…å™¨é›†æˆåˆ° LLM ä¸­
    - PEFT4ç¨®æ–¹æ³•
    - AdapterH
    - AdapterP
    - Parallel
      - å°†é€‚é…å™¨æ¨¡å—ä¸æ¯å±‚ Transformer çš„å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚å¹¶è¡Œè®¡ç®—é›†æˆã€‚ 
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)](https://zhuanlan.zhihu.com/p/621700272)
  - ä»‹ç´¹Huggingface - PEFT
  - ä»‹ç´¹LLM-Adapters 
### åŠ é€ŸæŠ€å·§
#### Optimizer

### äººé¡è¡Œç‚º
- [Using Large Language Models to Simulate Multiple Humans
and Replicate Human Subject Studies](https://arxiv.org/pdf/2208.10264)
  - ä½¿ç”¨GPT-3ä¾†æ¨¡æ“¬äººé¡çš„åæ‡‰ï¼Œé€šéæ”¹è®Šå§“åå’Œå…¶ä»–ç´°ç¯€ï¼Œåœ¨æŸäº›äººé¡å¯¦é©—ï¼ˆå¦‚çµ‚æ¥µåšå¼ˆï¼‰ä¸­é€²è¡Œæ¸¬è©¦ã€‚

### è§’è‰²äººæ ¼
- [Proxona: Leveraging LLM-Driven Personas to Enhance Creatorsâ€™ Understanding of Their Audience](https://arxiv.org/pdf/2408.10937)
  - åˆ©ç”¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”ŸæˆåŸºæ–¼æ•¸æ“šçš„å—çœ¾è§’è‰²ï¼Œå¹«åŠ©å‰µä½œè€…æ·±å…¥äº†è§£å…¶å—çœ¾ï¼Œä¸¦æ”¯æŒåˆ¶å®šä»¥å—çœ¾ç‚ºä¸­å¿ƒçš„å…§å®¹ç­–ç•¥ã€‚ 

### èªè¨€æ‡‰ç”¨
#### ç¬‘è©±ç†è§£
- [INNOVATIVE THINKING, INFINITE HUMOR: HUMOR RESEARCH OF LARGE LANGUAGE MODELS THROUGH STRUCTURED THOUGHT LEAPS](https://arxiv.org/pdf/2410.10370)
  - åŸºæ–¼å‰µæ„è·³èºæ€ç¶­CLoTæå‡ºä¸€å€‹**"å‰µæ„çµæ§‹åŒ–æ€ç¶­è·³èº"ï¼ˆCLoSTï¼‰æ¡†æ¶**
  - é¦–å…ˆï¼Œéœ€è¦ä¸€å€‹çå‹µæ¨¡å‹ä¾†é”æˆç³¾éŒ¯ç›®çš„(å› ç‚ºç›®å‰æ²’æœ‰å¹½é»˜çš„å°ˆå®¶æ¨¡å‹æˆ–å¯ç”¨è¦å‰‡ä¾†åˆ¤å®šå…§å®¹æ˜¯å¦å¹½é»˜)
  - é€šéå¼·åŒ–å­¸ç¿’ï¼Œæ¨¡å‹å­¸æœƒæ‰“ç£¨æ€ç¶­éˆçš„æ¨ç†ï¼Œä¸¦æ”¹é€²å…¶ä½¿ç”¨çš„ç­–ç•¥ã€‚
  - æ¨¡å‹å­¸æœƒè­˜åˆ¥ä¸¦ä¿®æ­£éŒ¯èª¤ï¼Œæœ€çµ‚ç”Ÿæˆæœ€å…·å¹½é»˜æ€§å’Œå‰µæ„çš„ç­”æ¡ˆã€‚
- [Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions](https://arxiv.org/pdf/2405.19088)
  - ç•¶å¹½é»˜æ¶‰åŠè¨±å¤šç¬‘è©±å’Œå¹½é»˜ç·šç´¢æ‰€ä¾è³´çš„éç·šæ€§æ•˜äº‹æ™‚ï¼ŒLLMåœ¨é€šéä¸¦åˆ—ç†è§£äººé¡å¹½é»˜çš„ç´°å¾®ä¹‹è™•ä»ç„¶é¢è‡¨å›°é›£
  - æå‡ºäº†**YESBUTåŸºæº–**ï¼ŒåŒ…å«ä¸åŒé›£åº¦çš„ä»»å‹™ï¼Œæ—¨åœ¨**è©•ä¼°AIè­˜åˆ¥å’Œè§£é‡‹é€™äº›æ¼«ç•«çš„èƒ½åŠ›**ï¼Œä»»å‹™ç¯„åœå¾å­—é¢å…§å®¹ç†è§£åˆ°æ·±å±¤æ•˜äº‹æ¨ç†ã€‚
    
#### æ‹’çµ•èƒ½åŠ›
- [â€œAs an AI language model, I cannotâ€: Investigating LLM Denials of User Requests](https://dl.acm.org/doi/pdf/10.1145/3613904.3642135)
- è¨è«–æœ‰é—œäº’å‹•ä¸­æ–·ã€ä¿®å¾©ç­–ç•¥çš„ç ”ç©¶ï¼Œä»¥åŠä¸­æ–·æ€§æºé€šå¦‚ä½•å½±éŸ¿ç”¨æˆ¶å°æ™ºèƒ½ç³»çµ±çš„æœŸæœ›å’Œç†è§£ï¼ˆä¾‹å¦‚ï¼Œè­¦å‘Šå’ŒéŒ¯èª¤ä¿¡æ¯ï¼‰ã€‚
  - æ­¸ç´2é¡æ‹’çµ•ç™¼ç”ŸåŸå› (å°‡é€™äº›éŒ¯èª¤ç¨±ç‚ºâ€œæ‹’çµ•â€)
    - technicalæŠ€è¡“æ€§åŸå› ï¼ˆå³ç³»çµ±ç„¡æ³•å®Œæˆè«‹æ±‚ï¼‰
      - è¨“ç·´æ•¸æ“šçš„ç©ºç™½ã€ç¼ºä¹å¯¦æ™‚æ•¸æ“šè¨ªå•ã€â€œæ•¸æ“šä¸å¯ç”¨â€ã€â€œä¸åœ¨è¨“ç·´æ•¸æ“šä¸­â€å’Œâ€œåƒ…é™æ–‡æœ¬è¼¸å‡ºâ€ï¼ˆä¾‹å¦‚ï¼Œç„¡æ³•è¨ªå•æˆ–è¨ªå•è¢«ç¦æ­¢ï¼‰, è¨“ç·´æ•¸æ“šç„¡æ³•è¶…éæˆªæ­¢æ—¥æœŸ
      - åŒ…å«ï¼šåŸºæº–æ‹’çµ•, è½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•, äº‹å¯¦æ‹’çµ•
      - ä¾†è‡ªç ”ç©¶ 1ï¼ˆæŠ€è¡“æ€§æ‹’çµ•ï¼‰çš„çµæœè¡¨æ˜ï¼Œèˆ‡åŸºæº–æ‹’çµ•ç›¸æ¯”ï¼Œåƒèˆ‡è€…ç™¼ç¾è½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•åœ¨æ²®å–ªç¨‹åº¦ä¸Šè¼ƒä½ï¼Œæ›´æœ‰ç”¨ã€æ›´é©ç•¶ä¸”æ›´ç›¸é—œã€‚
      - äº‹å¯¦æ€§æ‹’çµ•åœ¨æ‰€æœ‰è¡¡é‡æ¨™æº–ä¸Šï¼ˆé™¤äº†æ²®å–ªï¼‰éƒ½æ¯”è½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•è©•åˆ†ä½
      - åœ¨æŠ€è¡“æ€§æ‹’çµ•çš„æƒ…æ³ä¸‹ï¼Œå¸¶æœ‰æ„è¦‹çš„å›æ‡‰ä¸¦ä¸åˆé©ï¼Œå› ç‚ºå…¶æ ¹æœ¬åŸå› ä¸¦ä¸éœ€è¦è§£é‡‹ï¼ˆä¾‹å¦‚ï¼Œå°æ–¼æ•¸æ“šå®Œå…¨ç„¡æ³•è¨ªå•ï¼Œç„¡éœ€æœ‰é—œæ•¸æ“šçš„ä»»ä½•è¦‹è§£ï¼‰ã€‚
    - socialç¤¾æœƒæ€§åŸå› ï¼ˆå³ç³»çµ±è¢«ç¦æ­¢å®Œæˆè«‹æ±‚ï¼‰
      - ç¤¾æœƒæ€§æ‹’çµ•çš„åŸå› é€šå¸¸æ˜¯ç”±æ–¼æ”¿ç­–ï¼Œä¾‹å¦‚ä¸å…è¨± LLM ç”Ÿæˆä»‡æ¨æˆ–è‰²æƒ…å…§å®¹
      - åŒ…å«ï¼šåŸºæº–æ‹’çµ•, è½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•, å¸¶æœ‰æ„è¦‹çš„æ‹’çµ•
      - åœ¨ç ”ç©¶ 2ï¼ˆç¤¾æœƒæ€§æ‹’çµ•ï¼‰ä¸­ï¼Œæˆ‘å€‘ç™¼ç¾ï¼Œè½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•æ¯”åŸºæº–æ‹’çµ•å’Œå¸¶æœ‰æ„è¦‹çš„æ‹’çµ•åœ¨æ²®å–ªç¨‹åº¦ä¸Šè¼ƒä½ï¼Œä¸¦ä¸”æ›´æœ‰ç”¨ã€æ›´é©ç•¶å’Œæ›´ç›¸é—œã€‚
      - å¸¶æœ‰æ„è¦‹çš„æ‹’çµ•åœ¨æ‰€æœ‰è¡¡é‡æ¨™æº–ä¸Šæ¯”åŸºæº–æ‹’çµ•æ›´å—åƒèˆ‡è€…çš„æ­£é¢è©•åƒ¹ã€‚
      - å°‡äº‹å¯¦æ€§æ‹’çµ•é¢¨æ ¼å¾ç¤¾æœƒæ€§æ‹’çµ•ä¸­æ’é™¤ï¼Œå› ç‚ºé€™è£¡çš„äº‹å¯¦ç¸½æ˜¯ç›¸åŒçš„ï¼šæ”¿ç­–ä¸å…è¨±æä¾›å›æ‡‰ã€‚
  - 4ç¨®æ‹’çµ•é¢¨æ ¼
    - åŸºæº–æ‹’çµ•(baseline denials): é€™ç¨®æ‹’çµ•åƒ…ç°¡å–®åœ°è²æ˜ LLM ç„¡æ³•æä¾›å¹«åŠ©
    - äº‹å¯¦æ‹’çµ•(Factual denials): è©²é¢¨æ ¼æä¾›æ‹’çµ•ä¸¦é™„ä¸Šæ‹’çµ•çš„ç†ç”±
    - è½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•(diverting denials): LLM æœƒå¼•å°ç”¨æˆ¶åé›¢è«‹æ±‚
    - å¸¶æœ‰æ„è¦‹çš„æ‹’çµ•(opinionated denials): å›æ‡‰å¼·èª¿è«‹æ±‚çš„ä¸é©ç•¶æ€§ï¼Œ
  - å°æ–¼æ¯ç¨®æ‹’çµ•ï¼Œæˆ‘å€‘è©•ä¼°äº†ç”¨æˆ¶å°å…¶çš„æ²®å–ªç¨‹åº¦ã€æœ‰ç”¨æ€§ã€é©ç•¶æ€§å’Œç›¸é—œæ€§çš„æ„ŸçŸ¥ï¼Œä¸¦æ”¶é›†äº†ç”¨æˆ¶çš„é–‹æ”¾å¼åé¥‹ã€‚æˆ‘å€‘é€šéä¸åŒçš„ç³»çµ±æç¤ºä¾†å±•ç¾æ¯ç¨®æ‹’çµ•é¢¨æ ¼ï¼Œç¢ºä¿ LLM ç”Ÿæˆé©ç•¶çš„å›æ‡‰
  - ç”¨æˆ¶å°è½‰ç§»æ³¨æ„åŠ›çš„æ‹’çµ•çµ¦äºˆè¼ƒé«˜è©•åƒ¹ï¼Œæ˜¯å› ç‚ºå³ä½¿åŸå§‹è«‹æ±‚ç„¡æ³•æ»¿è¶³ï¼Œé€™äº›æ‹’çµ•ä»èƒ½æä¾›æœ‰åƒ¹å€¼çš„ä¿¡æ¯ã€‚
    - ä¾‹å¦‚é€šéæä¾›ä¸å†’çŠ¯çš„ç¬‘è©±ä¾†ä»£æ›¿è«‹æ±‚ä¸­çš„å†’çŠ¯æ€§ç¬‘è©±ã€‚   
    - ç¦®è²Œçš„èŠå¤©æ©Ÿå™¨äººä¹Ÿå¯èƒ½è¢«èªç‚ºæ˜¯â€œéæ–¼é“æ­‰ä¸”å¸¶æœ‰å±…é«˜è‡¨ä¸‹çš„èªæ°£â€
    - ä¾‹å¦‚ï¼Œå‡è¨­ä¸€å€‹ç”¨æˆ¶è¦æ±‚ LLM æä¾›ä¸ç•¶çš„ç¬‘è©±ã€‚èˆ‡å…¶æä¾›åƒâ€œè«‹æ±‚ä¸å…è¨±â€é€™æ¨£ç°¡çŸ­ä¸”ç¼ºä¹ä¿¡æ¯çš„å›æ‡‰ï¼ŒLLM æ‡‰è©²è¨­è¨ˆç‚ºé€™æ¨£èªªï¼šâ€œæˆ‘å€‘çš„å¹³å°è‡´åŠ›æ–¼ä¿ƒé€²ç©æ¥µå’ŒåŒ…å®¹çš„é«”é©—ï¼Œè€Œé€™å€‹è«‹æ±‚é•åäº†æˆ‘å€‘çš„æº–å‰‡ã€‚é€™è£¡æœ‰ä¸€å€‹ä½ å¯èƒ½æœƒå–œæ­¡çš„æ›¿ä»£ç¬‘è©±[......]â€
    - ç”¨æˆ¶å¯èƒ½ç„¡æ³•å­¸æœƒå¦‚ä½•èª¿æ•´ä»–å€‘çš„è«‹æ±‚ä»¥é¿å…æœªä¾†çš„ä¸­æ–·ã€‚
  - å»ºè­° LLM é¿å…åœ¨æœªçµ¦å‡ºç†ç”±çš„æƒ…æ³ä¸‹æ‹’çµ•ç”¨æˆ¶è«‹æ±‚ã€‚ç›¸åï¼Œå®ƒå€‘æ‡‰è©²æä¾›æœ‰é—œæ‹’çµ•åŸå› çš„é¡å¤–ä¿¡æ¯ï¼Œä¸¦å¯èƒ½å°‡æ‹’çµ•æƒ…å¢ƒåŒ–ã€‚
  - æä¾›ç„¡ä¿¡æ¯çš„å›æ‡‰ï¼ˆå³æˆ‘å€‘çš„åŸºæº–æƒ…å¢ƒï¼‰æ˜¯æœ€ä¸å—å–œæ„›çš„é¸é …ï¼Œå› ç‚ºé€™äº›æ‹’çµ•å¯èƒ½æœªèƒ½å‘ç”¨æˆ¶è§£é‡‹æ‹’çµ•çš„æ ¹æœ¬åŸå› ã€‚
  - æˆ‘å€‘å»ºè­° LLM é…å‚™ç³¾æ­£æ€§æ‹’çµ•ï¼ŒæŒ‡å°ç”¨æˆ¶æ”¹å–„ä»–å€‘çš„è«‹æ±‚æˆ–èª¿æ•´è¡Œç‚ºã€‚åƒèˆ‡è€…å°å¸¶æœ‰æ„è¦‹çš„æ‹’çµ•çµ¦äºˆäº†æ¯”åŸºæº–æ‹’çµ•æ›´é«˜çš„è©•åƒ¹ã€‚å¦‚å‰æ‰€è¿°ï¼Œå¸¶æœ‰æ„è¦‹çš„æ‹’çµ•ä½œç‚ºä¸€ç¨®ç³¾æ­£æ€§ä¿¡æ¯èµ·åˆ°äº†ä½œç”¨ã€‚
  
#### é›™é—œèª
- [Ambipun: Generating humorous puns with ambiguous context.](https://arxiv.org/abs/2205.01825)
- [â€œA good pun is its own rewordâ€: Can Large Language Models Understand Puns?](https://arxiv.org/pdf/2404.13599)
  - LLMå…¶ç†è§£é›™é—œèªçš„èƒ½åŠ›å°šæœªå¾—åˆ°ç³»çµ±æ€§ç ”ç©¶ï¼Œé€™é™åˆ¶äº†LLMsåœ¨å‰µæ„å¯«ä½œå’Œå¹½é»˜å‰µä½œä¸­çš„æ‡‰ç”¨
  - å¤§å¤šæ•¸LLMsèƒ½å¤ æº–ç¢ºè­˜åˆ¥åŒå½¢é›™é—œå’Œç•°å½¢é›™é—œä¸­çš„é›™é—œè©
  - é™¤GPT-4-Turboå’ŒClaude-3-Opuså¤–ï¼Œå…¶é¤˜LLMsåœ¨è­˜åˆ¥ç•°å½¢é›™é—œä¸­çš„æ›¿ä»£è© å’Œæ›¿ä»£æ„ç¾©æ™‚å­˜åœ¨å›°é›£ã€‚é€™ä¸€æŒ‘æˆ°æºæ–¼åœ¨ç•°å½¢é›™é—œä¸­ï¼Œä¸¦æœªç›´æ¥å‡ºç¾åœ¨æ–‡æœ¬ä¸­ï¼Œè€Œæ˜¯ä¾è³´æ–¼é€šéä¸Šä¸‹æ–‡å’Œç›¸ä¼¼æ€§é€²è¡Œçš„å–šèµ·ï¼Œé€™æ˜¯è§£é‡‹é›™é—œèªçš„åŸºç¤
  - ç•°å½¢é›™é—œçš„è­˜åˆ¥å¥½(æ›¿ä»£è©ä¸å½±éŸ¿é›™é—œèªçš„è­˜åˆ¥ï¼Œä½†å°æ­£ç¢ºè§£é‡‹é›™é—œèªè‡³é—œé‡è¦ã€‚)ï¼Œä½†ä¸ä¸€å®šèƒ½åšå‡ºå¥½çš„è§£é‡‹
  - å„˜ç®¡æˆ‘å€‘ä½¿ç”¨äº†ç›®å‰æœ€å»£æ³›ä½¿ç”¨çš„é›™é—œèªæ•¸æ“šé›†ä¾†è©•ä¼°LLMsçš„é›™é—œèªç†è§£èƒ½åŠ›ï¼Œä½†æˆ‘å€‘çš„é›™é—œæ–‡æœ¬å…¨éƒ¨ç‚ºè‹±æ–‡ã€‚LLMsç†è§£é›™é—œèªçš„èƒ½åŠ›å¯èƒ½å› èªè¨€è€Œç•°ï¼Œéè‹±èªèªè¨€çš„é›™é—œèªåœ¨å®šç¾©ã€çµæ§‹æˆ–ç”¨é€”ä¸Šå¯èƒ½æœ‰æ‰€ä¸åŒã€‚é€™ä¸€é™åˆ¶çªé¡¯äº†æœªä¾†å°‡ç ”ç©¶æ“´å±•åˆ°å…¶ä»–èªè¨€é›™é—œèªçš„æ½›åŠ›ã€‚
  - æ­ç¤ºäº†ã€Œ**æ‡¶æƒ°çš„é›™é—œç”Ÿæˆ**ã€æ¨¡å¼ï¼Œä¸¦ç¢ºå®šäº†LLMsåœ¨ç†è§£é›™é—œèªæ–¹é¢çš„ä¸»è¦æŒ‘æˆ°
    
#### ä¸­æ–‡ä¿šèª
- [DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding](https://arxiv.org/pdf/2405.15818)
  - æå‡ºäº†DuanzAIï¼Œä¸€ç¨®å¢å¼·å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ·±åº¦ç†è§£ä¸­æ–‡ä¿šèªçš„å‰µæ–°æ–¹æ³•ã€‚
  - åˆ©ç”¨ä¸‰å€‹å¸¸è¦‹ä»»å‹™ï¼Œå³**é›™é—œè­˜åˆ¥ã€é›™é—œè§£é‡‹å’Œé›™é—œç”Ÿæˆ**ï¼Œç³»çµ±**è©•ä¼°LLMsç†è§£é›™é—œèªçš„èƒ½åŠ›**ã€‚
  - 
#### joke
- [Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges](https://arxiv.org/pdf/2212.11214)
  - ä»¥**ä¸åŒå€‹æ€§é¢¨æ ¼LLMä¾†è©•æ¸¬ç¬‘è©±å“è³ª**
  - èªç‚ºå€‹æ€§çš„è¨­å®šéœ€è¦"è¶³ä»¥èª˜å°å‡ºå…·å‚™é€™äº›ç‰¹è³ªçš„å€‹æ€§" (ä¾‹ï¼šã€Œä½œç‚ºä¸€å€‹å–œæ­¡$TypeOfHumourå¹½é»˜çš„äººï¼Œå°‡ä»¥ä¸‹[Joke]åˆ†é¡ç‚ºFunnyæˆ–$Oppositeã€‚ã€)
  - 
##### Jokeå®šç¾©
- [Individual differences in uses of humor and their relation to psychological well-being: Development of the Humor Styles Questionnaire](https://www.sciencedirect.com/science/article/abs/pii/S0092656602005342)
  - **å®šç¾©ç¬‘è©±åˆ†é¡**ï¼šè¦ªå’Œå‹ã€è‡ªæˆ‘æå‡å‹ã€æ”»æ“Šå‹å’Œè‡ªæˆ‘è²¶ä½å‹
  - è¦ªå’Œå‹å¹½é»˜ï¼šä¸€ç¨®éæ•µå°ã€åŒ…å®¹çš„å¹½é»˜ï¼Œè‚¯å®šè‡ªæˆ‘èˆ‡ä»–äºº
  - è‡ªæˆ‘æå‡å‹å¹½é»˜ï¼šæ—¨åœ¨è®“äººå°è‡ªå·±æ„Ÿåˆ°è‰¯å¥½
  - æ”»æ“Šå‹å¹½é»˜ï¼šä½¿ç”¨è«·åˆºã€æ¶æ„ã€å˜²ç¬‘ã€è­è«·å’Œè²¶ä½ä»–äºº
  - è‡ªæˆ‘è²¶ä½å‹å¹½é»˜ï¼šæ˜¯ç‚ºäº†è®“ä»–äººé–‹å¿ƒè€Œè‡ªå˜²
  - 
##### Joke Dataset
- [Witscript: A System for Generating Improvised Jokes in a Conversation](https://computationalcreativity.net/iccc21/wp-content/uploads/2021/09/ICCC_2021_paper_15.pdf)
  - **ç¬‘è©±ç”±äºº(å°ˆæ¥­ç¬‘è©±æ¼”å“¡)èˆ‡LLM(GPT-3, Witscript, Witscript2)ç”Ÿæˆ**ï¼Œçš†å±¬æ–¼æ”»æ“Šæ€§/è‡ªæˆ‘è²¶ä½å‹
  - ç¬‘è©±ä¸¦éåƒ…ä¾è³´æ–‡å­—éŠæˆ²ï¼ŒåŸºæ–¼å¸¸è­˜
  - åŒæ™‚åŒ…å«äººé¡è©•å¯©å¹½é»˜è©•åˆ†
- [Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba](https://arxiv.org/pdf/2406.12754)
  - **Chumor è³‡æ–™é›†**ï¼š ä¾†æºæ–¼ã€Œå¼±æ™ºå§ã€ï¼ˆRZBï¼‰ï¼Œä¸€å€‹é¡ä¼¼Redditçš„ä¸­æ–‡å¹³å°ï¼Œå°ˆæ³¨æ–¼åˆ†äº«å…·æœ‰æ™ºåŠ›æŒ‘æˆ°æ€§å’Œæ–‡åŒ–ç‰¹æ€§çš„ç¬‘è©±
  - ç‚ºæ¯å€‹ç¬‘è©±æ·»åŠ äº†è§£é‡‹æ¨™è¨»ï¼Œä¸¦é€šéä¸­æ–‡æ¯èªè€…çš„A/Bæ¸¬è©¦
  - è³‡æ–™é›†ä¸‹è¼‰ï¼šhttps://github.com/dnaihao/Chumor-dataset
- [Is AI fun? HumorDB: a curated dataset and benchmark to investigate graphical humor](https://arxiv.org/pdf/2406.13564)
  - **HumorDB è³‡æ–™é›†**ï¼š æ–°å‹ç´”åœ–åƒæ•¸æ“šé›†ï¼Œæ—¨åœ¨æ¨å‹•è¦–è¦ºå¹½é»˜ç†è§£çš„é€²å±•
  - åŒ…å«ç²¾å¿ƒæŒ‘é¸çš„åœ–åƒå°ï¼Œå…·æœ‰å°æ¯”æ˜é¡¯çš„å¹½é»˜è©•åˆ†
  - çªé¡¯å¼•ç™¼å¹½é»˜çš„å¾®å¦™è¦–è¦ºç·šç´¢ï¼Œä¸¦æ¸›å°‘æ½›åœ¨çš„åè¦‹ã€‚è©²æ•¸æ“šé›†æ”¯æŒäºŒå…ƒåˆ†é¡ï¼ˆæœ‰è¶£æˆ–ç„¡è¶£ï¼‰ã€ç¯„åœå›æ­¸ï¼ˆå¹½é»˜ç¨‹åº¦å¾1åˆ°10ï¼‰å’Œæˆå°æ¯”è¼ƒä»»å‹™ï¼ˆå“ªå€‹åœ–åƒæ›´æœ‰è¶£ï¼‰
- [Are U a Joke Master? Pun Generation via Multi-Stage Curriculum Learning towards a Humor LLM](https://aclanthology.org/2024.findings-acl.51.pdf)
  - å°ˆæ³¨æ–¼è³¦äºˆLLMsç”Ÿæˆé›™é—œèªçš„èƒ½åŠ›ï¼Œé€™æ˜¯ä¸€ç¨®é€šéåå¥½å­¸ç¿’æ–¹æ³•å¯¦ç¾çš„ç‰¹å®šå¹½é»˜é¡å‹ã€‚
  - æå‡ºäº†ä¸€ç¨®å¤šéšæ®µèª²ç¨‹åå¥½å­¸ç¿’æ¡†æ¶ï¼Œä»¥å„ªåŒ–é›™é—œèªçµæ§‹åå¥½å’Œå¹½é»˜åå¥½ï¼Œæ”¹é€²äº†ç›´æ¥åå¥½å„ªåŒ–ï¼ˆDPOï¼‰ç®—æ³•ï¼Œä»¥è§£æ±ºå¤šç›®æ¨™å°é½Šå•é¡Œã€‚
  - çªé¡¯å¼•ç™¼å¹½é»˜çš„å¾®å¦™è¦–è¦ºç·šç´¢ï¼Œä¸¦æ¸›å°‘æ½›åœ¨çš„åè¦‹ã€‚è©²æ•¸æ“šé›†æ”¯æŒäºŒå…ƒåˆ†é¡ï¼ˆæœ‰è¶£æˆ–ç„¡è¶£ï¼‰ã€ç¯„åœå›æ­¸ï¼ˆå¹½é»˜ç¨‹åº¦å¾1åˆ°10ï¼‰å’Œæˆå°æ¯”è¼ƒä»»å‹™ï¼ˆå“ªå€‹åœ–åƒæ›´æœ‰è¶£
  - **ä¸­æ–‡é›™é—œèªï¼ˆChinesePunï¼‰æ•¸æ“šé›†**ï¼ŒåŒ…å«2.1ké›™é—œèªåŠå…¶å°æ‡‰æ¨™è¨»ã€‚
  - 
#### ç¬‘è©±ç”Ÿæˆ
- [Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models](https://arxiv.org/pdf/2403.00794)
  - æ¢è¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦èƒ½å¤ é€šéç·¨è¼¯æ–‡æœ¬ç”Ÿæˆå¹½é»˜æª¢æ¸¬çš„åˆæˆæ•¸æ“š
  - é¡¯ç¤ºç•¶å‰çš„LLMåœ¨ã€Œå»å¹½é»˜åŒ–ã€ç¬‘è©±æ–¹é¢è¡¨ç¾å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›
  - GPT-4ç”Ÿæˆçš„åˆæˆæ•¸æ“šå¾—åˆ°äº†é›™èªæ¨™è¨»è€…çš„é«˜åº¦è©•åƒ¹ï¼Œä¸¦ç‚ºå¹½é»˜åˆ†é¡å™¨æä¾›äº†å…·æŒ‘æˆ°æ€§çš„å°æŠ—ç¤ºä¾‹ã€‚
- [Exploring Chinese Humor Generation: A Study on Two-part Allegorical Sayings](https://arxiv.org/pdf/2403.10781)
  - æœ¬æ–‡ç ”ç©¶äº†æœ€å…ˆé€²çš„èªè¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆä¸­æ–‡å¹½é»˜æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ¥èšç„¦æ–¼è¨“ç·´å®ƒå€‘å‰µä½œè«·å–»èªå¥
  - ç”¨äº†å…©ç¨®ä¸»è¦çš„è¨“ç·´æ–¹æ³•ï¼šå°**ä¸­å‹èªè¨€æ¨¡å‹é€²è¡Œå¾®èª¿**å’Œå°**å¤§å‹æ¨¡å‹é€²è¡Œæç¤º**
  - å‰µæ–°å¾®èª¿æ–¹æ³•çµåˆäº†èåˆæ‹¼éŸ³åµŒå…¥ä»¥è€ƒæ…®åŒéŸ³è©ï¼Œä¸¦**ä½¿ç”¨å°æ¯”å­¸ç¿’èˆ‡åˆæˆçš„å›°é›£è² ä¾‹ä¾†å€åˆ†å¹½é»˜å…ƒç´ **

#### Layer Dropping
- [(2020)Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping](https://github.com/TroisLiu/research/blob/master/Accelerating%20Training%20of%20Transformer-Based%20Language%20Models%20with%20Progressive%20Layer%20Dropping.md)
#### Sparse Attention

#### Knowledge Distillation
- æŠ½å–è¤‡é›œæ¨¡å‹è¨“ç·´å‡ºçš„ç²¾è¯ç‚ºå¦ä¸€å€‹ç°¡å–®æ¨¡å‹æ‰€ç”¨ï¼Œè®“é€™å€‹å°çš„ç°¡å–®æ¨¡å‹ä¹Ÿèƒ½é”åˆ°è·Ÿè¤‡é›œæ¨¡å‹ä¸€æ¨£çš„æ•ˆæœ
  -  æ¡ç”¨teacher/studentå¸«å¾’æ¦‚å¿µçš„æ¡†æ¶ä¾†å¯¦ç¾ï¼Œç”±teacheræ¨¡å‹å…ˆè¨“ç·´å¥½æ¬Šé‡å¾Œï¼Œå†æŠ½å–ï¼ˆè’¸é¤¾ï¼‰ç²¾è¯ä½œç‚ºstudentæ¨¡å‹çš„è¨“ç·´æ•™æï¼Œè®“studentä¹Ÿèƒ½é”åˆ°æ¯”ç¾teacherçš„æ•ˆæœ
  -  ç²¾è¯æ˜¯æŒ‡è¨“ç·´å¥½çš„åƒæ•¸æ¬Šé‡ï¼Œä¹Ÿæœ‰ä¸€äº›äººç¨±ç‚ºDark knowledgeï¼ˆæš—çŸ¥è­˜ï¼‰ã€‚   
-  Geoffrey Hinton, Oriol VinyalsåŠJeff Deanåœ¨2015å¹´çš„è«–æ–‡ã€ŒDistilling the Knowledge in a Neural Networkã€ä¸­æ¨å»£
-  [(2022)LLM-QAT: Data-Free Quantization Aware Training for Large Language Models](https://github.com/TroisLiu/research/blob/master/LLM-QAT%3A%20Data-Free%20Quantization%20Aware%20Training%20for%20Large%20Language%20Models.md)
##### Reference
- [çŸ¥è­˜è’¸é¤¾ KnowledgeDistillation](https://chtseng.wordpress.com/2020/05/12/%E7%9F%A5%E8%AD%98%E8%92%B8%E9%A4%BE-knowledgedistillation/#:~:text=Knowledge%20Distillation%E4%B8%AD%E8%AD%AF%E7%82%BA,%E8%9B%BB%E8%AE%8A%E6%88%90%E7%82%BA%E7%BE%8E%E9%BA%97%E7%9A%84%E8%9D%B4%E8%9D%B6%EF%BC%8C%E3%80%82)
- 

### æ¨¡å‹
- [ç”Ÿæˆå¼ AI çš„æŠ€è¡“é–€æª»ï¼Œè­·åŸæ²³ä¸¦éä¸å¯é€¾è¶Š](https://hitripod.com/generative-ai-might-have-no-moat/)
#### DeepSeek-R1
- Pure RL Process: ç™¼ç¾ä¸ä½¿ç”¨SFTåšå†·å•Ÿå‹•ï¼Œä»¥RLåšå†·å•Ÿå‹•çš„æ•ˆæœä¹Ÿèƒ½å¾ˆå¥½
- ç¾¤é«”ç›¸å°ç­–ç•¥å„ªåŒ–ï¼ˆGroup Relative Policy Optimization, GRPO)
  - æ”¾æ£„äº†å‚³çµ± RL ä¸­èˆ‡ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰ç­‰å°ºå¯¸çš„è©•è«–æ¨¡å‹ï¼ˆCritic Modelï¼‰
  - æ”¹æ¡åŸºæ–¼ç¾¤é«”å¾—åˆ†ï¼ˆGroup Scoresï¼‰ä¾†ä¼°è¨ˆåŸºç·šå€¼ï¼Œå¾è€Œå¤§å¹…æ¸›å°‘è¨ˆç®—è³‡æºéœ€æ±‚ã€‚
  - é€éKL æ•£åº¦æ‡²ç½°ï¼ˆKL Divergence Penaltyï¼‰ä¾†èª¿æ§æ–°ç­–ç•¥èˆ‡åƒè€ƒç­–ç•¥ä¹‹é–“çš„å·®ç•°ï¼Œä½¿å­¸ç¿’éç¨‹æ›´åŠ ç©©å®š
  - ä½¿ RL è¨“ç·´èƒ½å¤ åœ¨ç„¡éœ€è©•è«–æ¨¡å‹çš„æƒ…æ³ä¸‹ï¼Œé€éç¾¤é«”æ¨£æœ¬çš„çµ±è¨ˆç‰¹æ€§ä¾†å¯¦ç¾é«˜æ•ˆçš„ç­–ç•¥å„ªåŒ–ï¼Œæé«˜æ¨ç†èƒ½åŠ›çš„å­¸ç¿’æ•ˆç‡
  - ç”¨äº†åŸºæ–¼è¦å‰‡çš„çå‹µç³»çµ±ï¼ˆRule-Based Reward Systemï¼‰
    - æº–ç¢ºæ€§çå‹µ(E.G.æ•¸å­¸æœ‰å›ºå®š/æ˜ç¢ºç­”æ¡ˆ)
    - æ ¼å¼çå‹µ
- RL with Cold-Start
  - å°‘é‡é«˜å“è³ªçš„é•·æ€ç¶­éˆï¼ˆLong Chain-of-Thought, CoTï¼‰æ•¸æ“šé€²è¡Œå†·å•Ÿå‹•
    - æ•¸åƒæ¢ 
  - 4éšæ®µ 
    - å†·å•Ÿå‹•éšæ®µï¼ˆCold Start Phaseï¼‰ï¼šå¼•å…¥å°‘é‡é«˜å“è³ªæ•¸æ“šä»¥æä¾›åˆå§‹æŒ‡å°ã€‚
    - ç¬¬ä¸€éšæ®µå¢å¼·å­¸ç¿’ï¼ˆFirst RL Stageï¼‰ï¼šæ¢ç´¢æ›´å„ªåŒ–çš„æ¨ç†æ¨¡å¼ï¼Œæå‡åˆæ­¥æ¨ç†èƒ½åŠ›ã€‚
    - ç›£ç£å¾®èª¿éšæ®µï¼ˆSupervised Fine-Tuning, SFTï¼‰ï¼šåˆ©ç”¨ç¶“éç¯©é¸çš„æ•¸æ“šé€²ä¸€æ­¥å°é½Šäººé¡åå¥½ï¼Œå¢å¼·å¯è®€æ€§èˆ‡é€šç”¨æ€§ã€‚
    - ç¬¬äºŒéšæ®µå¢å¼·å­¸ç¿’ï¼ˆSecond RL Stageï¼‰ï¼šåœ¨æ›´å¼·å¤§çš„åŸºç¤ä¸Šé€²ä¸€æ­¥å„ªåŒ–æ¨ç†æ€§èƒ½ï¼Œä½¿æ¨¡å‹å…·å‚™æ›´é«˜ç´šçš„æ¨ç†èƒ½åŠ›èˆ‡é©æ‡‰æ€§ã€‚    

##### Reference
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)
- [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)
- [ã€LLM å°ˆæ¬„ã€‘Deepseek v3 çš„è¨“ç·´æ™‚é–“åˆ°åº•åˆä¸åˆç†ï¼Ÿæ·ºè«‡ LLM Training efficiency](https://axk51013.medium.com/deepseek-v3-%E7%9A%84%E8%A8%93%E7%B7%B4%E6%99%82%E9%96%93%E5%88%B0%E5%BA%95%E5%90%88%E4%B8%8D%E5%90%88%E7%90%86-%E6%B7%BA%E8%AB%87-llm-training-6460374b6d0f)
- [ä¸ï¼Œä½ ç„¡æ³•ç”¨ 600 è¬ç¾å…ƒè¤‡è£½ä¸€å€‹ DeepSeek R1](https://technews.tw/2025/01/28/you-cannot-copy-deepseekr1-with-6m)
  - DeepSeek R1
    - R1 æ¨¡å‹çš„è¨“ç·´è²»ç”¨å…¶å¯¦å’Œå»å¹´åº•ç™¼ä½ˆçš„ V3 æ¨¡å‹ç›¸åŒ
  - DeepSeek V3
    - V3 æ¨¡å‹ä¸­çš„å¤šæ•¸åŠŸèƒ½åˆå’Œ 2024 å¹´åˆç™¼ä½ˆçš„ V2 æ¨¡å‹å…±ç”¨
    - è² è¼‰å¹³è¡¡
    - å¤šé‡ token é æ¸¬æ©Ÿåˆ¶
  - DeepSeek V2
    - DeepSeekMoE: å¤šé‡å°ˆå®¶æ··åˆï¼ˆMixture of Expertsï¼‰
    - DeepSeekMLA: å¤šé ­æ½›åœ¨æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆMulti-Head Latent Attentionï¼‰
- [Timeline of DeepSeek Papers on arXiv](https://medium.com/@fangkuoyu/timeline-of-deepseek-papers-on-arxiv-44ba1935f714https://medium.com/@fangkuoyu/timeline-of-deepseek-papers-on-arxiv-44ba1935f714)

#### DeepSeekMoE
- ç›®æ¨™ï¼šå¯¦ç¾æ›´é«˜æ•ˆçš„å°ˆå®¶ç‰¹åŒ–ï¼Œæé«˜ MoE æ¨¡å‹çš„çŸ¥è­˜åˆ©ç”¨æ•ˆç‡ï¼Œä¸¦æ¸›å°‘å†—é¤˜è¨ˆç®—æˆæœ¬ã€‚
- å‚³çµ±MoE
  - å‚³çµ± MoE æ¶æ§‹ä¸­ï¼ŒTransformer çš„å‰é¥‹ç¥ç¶“ç¶²è·¯ï¼ˆFeed-Forward Networks, FFNsï¼‰é€šå¸¸è¢« MoE å±¤ï¼ˆMoE Layersï¼‰ å–ä»£ã€‚
  - æ¯å€‹ MoE å±¤ ç”±å¤šå€‹å°ˆå®¶çµ„æˆï¼Œæ¯å€‹å°ˆå®¶çš„çµæ§‹èˆ‡æ¨™æº– FFN ç›¸åŒ
  - è€Œæ¯å€‹ token æœƒè¢«åˆ†é…çµ¦ä¸€å€‹ï¼ˆFedus et al., 2021ï¼‰æˆ–å…©å€‹ï¼ˆLepikhin et al., 2021ï¼‰å°ˆå®¶ã€‚
  - æœ‰å…©å¤§ç¼ºé™·é˜»ç¤™äº† å°ˆå®¶ç‰¹åŒ–ï¼ˆExpert Specializationï¼‰ï¼šè®“æ¯å€‹å°ˆå®¶å­¸ç¿’éé‡ç–Šä¸”é«˜åº¦èšç„¦çš„çŸ¥è­˜ã€‚  
- å‚³çµ±MoEçš„ç¼ºé™·ï¼š
  - çŸ¥è­˜æ··é›œï¼ˆKnowledge Hybridity)
    - ç¾æœ‰ MoE æ–¹æ³•é€šå¸¸ä½¿ç”¨æœ‰é™æ•¸é‡çš„å°ˆå®¶ï¼ˆä¾‹å¦‚ 8 æˆ– 16 å€‹ï¼‰ï¼Œå› æ­¤è¢«åˆ†é…åˆ°æŸå€‹å°ˆå®¶çš„ token å¯èƒ½æ¶‰åŠå¤šç¨®ä¸åŒçš„çŸ¥è­˜é ˜åŸŸã€‚
    - å–®å€‹å°ˆå®¶éœ€è¦å­˜å„²å¤§é‡ä¸åŒé¡å‹çš„çŸ¥è­˜ï¼Œé€™äº›çŸ¥è­˜å¯èƒ½ç„¡æ³•åœ¨æ¨ç†æ™‚åŒæ™‚é«˜æ•ˆåˆ©ç”¨ï¼Œå¾è€Œå½±éŸ¿æ¨¡å‹çš„æ€§èƒ½ã€‚
  - çŸ¥è­˜å†—é¤˜ï¼ˆKnowledge Redundancy
    - è¢«åˆ†é…åˆ°ä¸åŒå°ˆå®¶çš„ token å¯èƒ½ä»ç„¶éœ€è¦å…±äº«æŸäº›å¸¸è¦‹çŸ¥è­˜ï¼ˆCommon Knowledgeï¼‰
    - å¤šå€‹å°ˆå®¶å¯èƒ½æœƒåœ¨å„è‡ªçš„åƒæ•¸ä¸­å­¸ç¿’é‡è¤‡çš„çŸ¥è­˜ï¼Œå°è‡´å°ˆå®¶å±¤çš„åƒæ•¸å†—é¤˜ï¼ˆParameter Redundancyï¼‰ï¼Œé€²ä¸€æ­¥å½±éŸ¿æ¨¡å‹çš„æ•ˆç‡èˆ‡æ¨ç†èƒ½åŠ›ã€‚
- 3å€‹é—œéµæ¶æ§‹
  - ç²¾ç´°åŠƒåˆ†å°ˆå®¶ï¼ˆFine Segmentation of Experts) : å°‡å°ˆå®¶å¾ N ç´°åˆ†ç‚º ğ‘šğ‘ å€‹ï¼Œä¸¦å¾ä¸­æ¿€æ´» ğ‘šğ¾ å€‹
    - ä¿æŒç¸½åƒæ•¸æ•¸é‡ä¸è®Šçš„å‰æä¸‹ï¼Œæˆ‘å€‘é€éåŠƒåˆ† FFNï¼ˆå‰é¥‹ç¥ç¶“ç¶²è·¯ï¼‰çš„ä¸­é–“éš±è—ç¶­åº¦ï¼Œå°‡å°ˆå®¶é€²è¡Œæ›´ç´°ç²’åº¦çš„åŠƒåˆ†
    - ä¿æŒç¸½è¨ˆç®—æˆæœ¬ä¸è®Šçš„æ¢ä»¶ä¸‹ï¼Œæˆ‘å€‘æ¿€æ´»æ›´å¤šçš„ç´°ç²’åº¦å°ˆå®¶ï¼Œå¾è€Œå½¢æˆæ›´éˆæ´»ä¸”é©æ‡‰æ€§æ›´å¼·çš„å°ˆå®¶çµ„åˆã€‚
      - ç•¶N=16ï¼Œè‹¥è·¯ç”±ç­–ç•¥ä¸€æ¬¡é¸TOP-2 => C16å–2 > 120ç¨®çµ„åˆ
      - ç•¶N=64ï¼Œè·¯ç”±ç­–ç•¥èª¿æ•´ç‚ºä¸€æ¬¡é¸TOP-8 => C64å–8 > 4,426,165,368ç¨® 
  - å…±äº«å°ˆå®¶éš”é›¢(Isolation of Shared Experts):å°‡ğ¾ğ‘ å€‹å°ˆå®¶ä½œç‚ºå…±äº«å°ˆå®¶(Shared Experts)ç”¨æ–¼å­¸ç¿’é€šç”¨çŸ¥è­˜ï¼Œä»¥æ¸›å°‘è·¯ç”±å°ˆå®¶(Routed Experts)ä¹‹é–“çš„å†—é¤˜
    - å°‡éƒ¨åˆ†å°ˆå®¶éš”é›¢ç‚ºã€Œå…±äº«å°ˆå®¶ï¼ˆShared Expertsï¼‰ã€
    - Shared Expertså§‹çµ‚è¢«æ¿€æ´»
      - ç”¨æ–¼æ•æ‰å’Œæ•´åˆé€šç”¨çŸ¥è­˜ï¼ˆCommon Knowledgeï¼‰
      - æ‰€æœ‰ token éƒ½æœƒè¢«ç¢ºå®šæ€§åœ°åˆ†é…åˆ°é€™äº›å°ˆå®¶ã€‚
    - ç¢ºä¿è·¯ç”±å°ˆå®¶ï¼ˆRouted Expertsï¼‰èƒ½å¤ å°ˆæ³¨æ–¼å­¸ç¿’ç¨ç‰¹ä¸”å°ˆç²¾çš„çŸ¥è­˜
  - è² è¼‰å‡è¡¡è€ƒé‡ (Load Balance Consideration): ç‚ºäº†é¿å…ä»¥ä¸‹2ç¼ºé™·
    - è·¯ç”±å´©æ½°ï¼ˆRouting Collapseï¼‰ï¼šæ¨¡å‹å¯èƒ½ç¸½æ˜¯é¸æ“‡å°‘æ•¸å¹¾å€‹å°ˆå®¶ï¼Œå°è‡´å…¶ä»–å°ˆå®¶ç„¡æ³•ç²å¾—è¶³å¤ çš„è¨“ç·´æ©Ÿæœƒï¼Œå¾è€Œå½±éŸ¿æ•´é«”çš„å°ˆå®¶å¤šæ¨£æ€§èˆ‡æ³›åŒ–èƒ½åŠ›ã€‚
    - è¨ˆç®—ç“¶é ¸ï¼ˆComputation Bottlenecksï¼‰ï¼šå¦‚æœå°ˆå®¶åˆ†ä½ˆåœ¨å¤šå€‹è¨­å‚™ä¸Šï¼Œè² è¼‰ä¸å‡è¡¡å¯èƒ½å°è‡´æŸäº›è¨­å‚™çš„è¨ˆç®—è³‡æºéè¼‰
#### LLaMA
- LongLLaMA
  - [å°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ° 256kï¼Œæ— é™ä¸Šä¸‹æ–‡ç‰ˆæœ¬çš„OpenLLaMAæ¥äº†ï¼Ÿ](https://www.jiqizhixin.com/articles/2023-07-10-3)
  - åŸºæ–¼OpenLLaMAå¯¦ä½œ
  - å¯å•†ç”¨
  - æ¡ç”¨FOT ï¼ˆ Focused Transformer )å¾®èª¿
#### LLaMA2
#### GPT
#### Bloomz

### è³‡æ–™é›†
- å¾…é–±è®€
  - [(2021)Deduplicating training data makes language models better](https://arxiv.org/pdf/2107.06499.pdf)
    - å»é™¤è³‡æ–™ä¸­é‡è¤‡çš„éƒ¨åˆ†
  - [Scaling Data-Constrained Language Models](https://arxiv.org/pdf/2305.16264.pdf)
    - å‘½é¡Œï¼šç•¶è³‡æ–™ä¸å¤ ï¼Œèƒ½åšä»€éº¼?
    - Focus: quantify the impact of multiple epochs in LLM training such that practitioners can decide how to allocate compute when scaling models
    - æå‡º data-constrained scaling law, é€šç”¨åŒ– Chinchilla scaling law
    - å„˜ç®¡åƒ…è¨“ç·´å–®ä¸€epochçš„æ¨¡å‹å§‹çµ‚å…·æœ‰æœ€ä½³çš„é©—è­‰æå¤±å’Œè¨ˆç®—æ•ˆç›Šï¼Œä½†åœ¨è¨“ç·´å¤šé”4å€‹epochçš„æ¨¡å‹ä¹‹é–“çš„å·®ç•°å¾€å¾€ä¸é‡è¦ï¼Œä¸¦ä¸”ä¸æœƒå°è‡´ä¸‹æ¸¸ä»»å‹™æ€§èƒ½çš„å·®ç•°ã€‚
    - æŒçºŒè¨“ç·´é¡å¤–çš„epochæ˜¯æœ‰ç›Šçš„ï¼Œä½†å›å ±æœ€çµ‚æœƒè¶¨è¿‘æ–¼é›¶ã€‚
    - åœ¨å—é™çš„æ•¸æ“šæƒ…å¢ƒä¸‹ï¼Œå°‡æ–°çš„è¨ˆç®—è³‡æºåˆ†é…çµ¦æ›´å¤šçš„åƒæ•¸å’Œæ™‚æœŸæ˜¯å¿…è¦çš„ï¼Œä¸¦ä¸”æ™‚æœŸæ‡‰è©²ç¨å¾®åŠ å¿«
#### MMLU
- 57å€‹å­¸ç§‘é ˜åŸŸ
- [è¦å¦‚ä½•è©•ä¼°å¤§å‹èªè¨€æ¨¡å‹çš„é æ¸¬èƒ½åŠ›ï¼Ÿé–‹ç®±å¤§å‹èªè¨€æ¨¡å‹è©•ä¼°ç¨‹å¼ç¢¼](https://blog.infuseai.io/llm-model-evaluation-open-source-code-mmlu-tmmluplus-mistral-c84828178114)
- [MMLU æ˜¯ä»€éº¼](https://ai.choozmo.com/blog/what-is-mmlu-dataset/)
#### TMMLU+
- 66å€‹å­¸ç§‘é ˜åŸŸ
- [TMMLU+ Dataset: å¤§è¦æ¨¡ç¹é«”ä¸­æ–‡æµ·é‡å¤šä»»å‹™èªè¨€ç†è§£](https://blog.infuseai.io/tmmluplus-dataset-brief-introduction-ecfd00297838)
- [TMMLU+: An Improved Traditional Chinese Evaluation Suite
for Foundation Models](https://arxiv.org/pdf/2403.01858v3)


## Recommendation System
- [Recommender System.md](https://github.com/TroisLiu/research/blob/master/Recommender%20System.md)

### Survey
- [(2022)A Survey of Recommendation Systems: Recommendation Models, Techniques, and Application Fields](https://github.com/TroisLiu/research/blob/master/A%20Survey%20of%20Recommendation%20Systems%3A%20Recommendation%20Models%2C%20Techniques%2C%20and%20Application%20Fields.md)

## Natural Language Processing
### Entity Linking

## life-long learning
- [(2022)Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora](https://aclanthology.org/2022.naacl-main.351.pdf)
  - 2ç¨®ä¸åŒé¡å‹çš„çŸ¥è­˜è½‰ç§»
    - é ˜åŸŸè½‰ç§»
    - æ™‚é–“éé€²
- [(2023)CONTINUAL PRE-TRAINING OF LANGUAGE MODELs](https://openreview.net/pdf?id=m_GDIItaI3o)
- [(2020)LAMOL: LANGUAGE MODELING FOR LIFELONG LANGUAGE LEARNING](https://arxiv.org/pdf/1909.03329.pdf)
  - Memory Replayæ‰‹æ³•
  - gpt-2ç‚ºåŸºåº•

### reference
- [ã€2023Q2ã€‘LLMç‚¼ä¸¹trickæ‹¾é—ï¼šLLMçš„MoEæ¶æ„ä¸Lifelong learning](https://zhuanlan.zhihu.com/p/636604779)
  - Mixure of Expert, MoE 
