# Dialogue System
## 依據對話目的分類
### 開放域對話（open-domain dialogue）
- 目標：
  - 需要展現出知識淵博、風趣且語言自然的特性
  - 追求對話的豐富性和連貫性，比如閒聊、問答、娛樂
  - 避免不當言論
  - 強調個性化和情感溝通等品質
- LLM：
  - 優點：經過大規模預訓練的模型本身掌握了廣泛的常識和知識，能夠就各種話題侃侃而談，並產生類似人類風格的回應
  - 缺點：容易天馬行空：例如對於未知領域的提問，模型可能不願承認自己的不知道，而編造看似合理但實則錯誤的答案（即幻覺問題）
  - 
### 任務導向對話（task-oriented dialogue, TOD）
- 目標：
  - 聚焦在完成特定目標上
  - 如: 預訂餐廳、查詢列車時刻
  - 需要與後端資料庫或API交互，遵循嚴格的業務邏輯
  - 要求對話精確且高效
- LLM：
  - 優點：讓LLM直接根據對話歷史生成對話行為和槽位填充結果，再據此產生回應，減少模組誤差傳遞
  - 缺點：容易天馬行空：例如對於未知領域的提問，模型可能不願承認自己的不知道，而編造看似合理但實則錯誤的答案（即幻覺問題）
- 作法
  - LLM在對話中自動決定何時調用API並填入參數，執行如資料庫查詢、計算等操作後再繼續對話
  - 混合式系統成為熱門方案：讓LLM處理自然語言理解和生成部分，但關鍵決策仍由顯式策略模組或規則把關
- 挑戰：
  - 當任務流程複雜或需要遵循業務政策時，僅靠在Prompt中添加大量指示往往不足以約束LLM的行為
  - LLM在任務型對話中的幻覺和不服從問題仍需關注
  - LLM在嚴格遵守對話流程上仍缺乏機制，開發者難以完全控制其每一步行為
### 融合風格對話
- 在主要任務流程中穿插寒暄和安撫，用戶既能得到任務結果也有良好體驗​
- 挑戰：
  - 如何動態調節兩種對話模式的切換、確保不互相干擾
- 
## 相關研究議題
### 對話狀態追蹤(Dialogue State Tracking, DST)
- LLM較善於利用完整對話語境來糾正先前輪次可能出現的錯誤
- LLM-DST的整體性能隨對話輪數增加而下降得更慢，對錯誤傳播（error propagation）的抵抗力比傳統模型更佳
- 核心議題
  - 共指 Coreference
    - 指多個詞或短語在語境中指向同一個實體
    - Coreference Resolution是提升 DST 表現時的一大障礙：多輪對話中語言表達多樣性所導致，槽位與其值經常是以間接/代名詞方式表達
    - 對應議題
      - 代詞消解（Pronoun Resolution）: 處理「他、她、它、他們」等代詞指涉誰的問題
      - 命名實體共指（Named Entity Coreference）: 識別像「台達電子」與「Delta」、「台達電子工業股份有限公司」是否指同一個組織
      - 跨句共指（Cross-sentence Coreference）: 不同句子中指涉同一實體的辨識
      - 事件共指（Event Coreference）: 識別不同語句是否指的是同一個事件
      - 共指鏈（Coreference Chains）: 將所有指涉同一實體的詞彙串成一條鏈，供後續理解與推理使用
  - 錯誤傳播 Error Propagation
  - ![image](https://github.com/user-attachments/assets/9c2479a1-0860-4820-964f-f9360f72c129)

- 用對話狀態（如槽位-值集合）作為對話歷史的高度摘要
- 論文：
  - [(2022)"Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking](https://arxiv.org/abs/2207.14627)
  - [(2023)Towards LLM-driven Dialogue State Tracking](https://aclanthology.org/2023.emnlp-main.48.pdf)
    - 貢獻:
      - 以GPT-3.5做實驗(擷取關鍵資訊)發現：
        - Prompt中若提供範例，對話範例會影響GPT取得關鍵資訊的正確率
        - 同時問多個Slot也會影響準確率
      - 提出LDST框架: 基於開源LLaMA模型驅動DST，包含"組合式領域-槽位指令微調(assembled domainslot instruction tuning method)"和"參數高效微調技術(parameter efficient tuning technique)"實現
    - 構建指令資料集
      - 以指令微調手法(針對任務設計的明確且具體的指令)來引導模型
      - 指令資料集
        - 以組合式領域-槽位指令生成法（Assembled Domain-Slot Instruction Generation)建構：透過隨機組合不同的指令模板與輸入模板來產生多樣化的指令樣本，讓模型在微調過程中接觸到各種形式的指令，有助於降低其對單一提示樣式的敏感度，提升泛化能力
        - 每一筆資料包含三個欄位：
          -  Instruction（任務指令）：會隨機二選一建構指令
            - 標準槽位追蹤指令（Standard Slot Tracking Instruction）
            - 自訂槽位追蹤指令（Customized Slot Tracking Instruction）：包含更具體的領域-槽位資訊
          -  Input（任務輸入）：由以下4部分組成
            - 對話上下文（dialogue context）：包含特殊區段Token: [USER], [SYSTEM]
            - 領域-槽位描述提示（domain-slot description prompt）：包含特殊區段Token: [domain], [slot]
            - 可能值列表（PVL, Possible Value List）提示：僅用於分類類型的槽位（categorical slots）
            - 查詢提示（query prompt）
          -  Output（期望輸出）
          -  提示詞設計是基於人工經驗的主觀選擇
        - 生成資料時，各有50%的機率生成"欠缺slot描述"/"欠缺可能值列表"的Input prompt
        - ![image](https://github.com/user-attachments/assets/b2355864-ce0e-4919-827f-271fa6407487)
      - 以PEFT方法微調模型
        - 以LoRA作微調
        - 基底模型: LLaMa 7B
        - 可學習參數：8.4M (總參數量的0.12%)
    - 資料集
      - ![image](https://github.com/user-attachments/assets/fd301ff8-22fc-46c8-95a8-3595961838f9)
      - Schema-Guided Dialogue(SGD)
        - 2020推出
        - 涵蓋16個領域、26個服務"
      - MultiWOZ 2.2 與 MultiWOZ 2.4
        - 2.2是2020推出；2.4是2022推出
        - 目的是提升 DST 任務的評估準確性
        - 2.4 版本的驗證集與測試集經過了精確的重新標註
    - 評估指標
      - Joint Goal Accuracy（JGA）
        - 模型在每一輪對話中**是否能完整正確地預測整個對話狀態（所有槽位的預測值）的比例**
        - DST 任務中的主要評估指標
        - 只有當一輪中的所有槽位都預測正確時，該輪才會被計入準確率
      - Average Goal Accuracy（AGA）
        - AGA 是每輪中**所有「啟用槽位（active slots）」**的平均準確率。
        - 啟用槽位: 指在當前輪對話中被提及，不是從前一輪繼承而來的槽位
    - 表現
      - ![image](https://github.com/user-attachments/assets/886bd8f2-6b1e-48bd-bd7d-a04cc6812eb4)
      - ![image](https://github.com/user-attachments/assets/94831723-f0dd-4034-a7fc-a86a2ee7de02)
      - ![image](https://github.com/user-attachments/assets/120e5116-720d-4604-92b4-8f77d30c3ff6)
      - ![image](https://github.com/user-attachments/assets/d5a6a37a-8f5d-4050-9e95-15c37f627621)
 
      - Error Propogation議題
        - ![image](https://github.com/user-attachments/assets/d8278b4d-52c6-4573-8354-00acf769d811)
        - LDST 的下降速度遠低於 LLaMa 與最佳基線方法，顯示其對錯誤傳遞的抵抗能力更強
    - 其他議題：
      - 當對話或描述內容過長時，如何有效截斷或摘要化輸入內容亦成為一項挑戰   
  - [(2024)Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation](https://aclanthology.org/2024.acl-long.473.pdf)
    - 因人工標註成本高，利用LLM來模擬對話生成標註數據
    - 論文使用GPT-4充當用戶和代理人，生成帶有DST標籤的大量模擬對話，然後用這些合成數據對LLaMA 2模型進行兩階段微調，在MultiWOZ等資料集上取得優於僅用真實數據訓練的效果
  - [(2024)Large Language Models as Zero-shot Dialogue State Tracker through Function Calling](https://arxiv.org/abs/2402.10466)
  - [(2025) Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs](https://arxiv.org/abs/2503.08857)
### RL與對話決策
- 類別
  - RLHF
    - [(2019)Reinforcement Learning for Personalized Dialogue Management](https://arxiv.org/abs/1908.00286)
      - 可能太舊 
    - [(2024)Knowledge acquisition for dialogue agents using reinforcement learning on graph representationsKnowledge acquisition for dialogue agents using reinforcement learning on graph representations](https://arxiv.org/html/2406.19500v1)
  - RLAIF 
- 論文：
  - [(2024)Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents](https://openreview.net/pdf?id=MCNqgUFTHI)
    - 讓一個可調參的小型策略模型作為插件指導LLM的對話決策
    - 以有標註數據進行監督微調，隨後採用自我對弈(self-play)方式讓LLM與自身模擬對話，透過AI代理提供的目標導向反饋進行強化學習​
    - 經過RL微調的策略插件可以顯著提升LLM在主動式對話任務，使LLM在沒有人工介入的情況下，不斷改進對話策略，並能快速遷移到新場景
### 外部知識增強對話管理
- 透過RAG改善LLM幻覺議題
- 這在任務型問答、推薦對話系統當中會很有幫助
- 系統
  - stanford的KITA
  - ChatCRS：工具增強的知識檢索代理、目標規劃代理
- 論文
  - [(2024)Knowledge-enhanced Response Generation in Dialogue Systems: Current Advancements and Emerging Horizons](https://aclanthology.org/2024.lrec-tutorials.13.pdf) 
### 上下文管理與長程對話
- LLM受限於有限的上下文視窗，在特別長的對話仍面臨挑戰
- 多數模型在輸入超過數千Token後，性能會下降，難以掌握對話中跨越很多倫次的因果關係、時間脈絡
- 資料集
  - LoCoMo長程對話資料集
    - 每段對話300輪次，9K個詞以上，有數十次獨立對話
    - 用來評估LLM長程記憶能力
- 對應策略
  - 增加模型上下文窗口：但可能丟失對話中間內容
  - 動態檢索相關歷史：
    - 參考RAG概念，將超出上下文窗口的歷史對話內容存入向量化資料庫
    - 產生回應時，透過檢索相關的過往語句，將相關內容附加給LLM)
  - 摘要與壓縮歷史：
    - 隨著對話進行，不斷壓縮舊對話內容生成摘要
    - 提取出對話狀態，以壓縮表示方式融入上下文
  - 顯式對話記錄與狀態跟蹤
    - 構建一個外部記憶來存儲對話中的關鍵資訊或狀態，舉例
     - 之前已確認的事實
     - 用戶偏好
    - 每次需要產生回應時，將使用者輸入及相關資訊提供給模型
- 方法:
  - [(2024)StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses](https://openreview.net/pdf?id=eNvVjpx97O)
    - 利用對話中特殊的「發話結束」標記（End-of-Utterance）作為會話注意匯聚點
    - 將長長的對話歷史壓縮到這些標記上，儘可能保留關鍵資訊
    - 其他策略，減少壓縮帶來的信息損失​
      - 短期記憶重構
      - 長期記憶再激活
    - 大幅降低隨著對話輪數增加的計算成本
    - 模型能處理超過數百輪對話而保持良好效果

### 以推理引擎增強對話管理
- 論文
  - [(2024)Chain of Thought Explanation for Dialogue State Tracking](https://arxiv.org/html/2403.04656v1)
    - 貢獻
      - 提出模型
        - Chain-of-Thought-Explanation, CoTE
        - CoTE-refined: CoTE + 自動同義改寫（paraphrasing）機制
      - 資料集
        - MultiWOZ 2.2
        - WoZ 2.0 (2017): 來自 Wizard-of-Oz 實驗的資料集，使用者透過打字輸入查詢（非語音輸入），因此對話上下文更為複雜。
        - M2M
          - 包含來自兩個領域（餐廳與電影）的 3,000 筆模擬對話，後續經人工校正。
          - 將餐廳與電影子資料集分別稱為 M2M-R 和 M2M-M，合併版則記為 M2M-R+M。 
    - 概念
      - 將CoT引入至DST當中，模型在決定槽位值後生成逐步推理的解釋
        - ![image](https://github.com/user-attachments/assets/03259e63-8067-473d-af85-db8f73b0c0b4)

        -  粗略解釋（Coarse Explanation)
        -  精緻解釋（Refined Explanation）
      - 能引導模型從相關對話輪中蒐集資訊並推理正確的槽值，從而提高預測的準確可靠性
    - 模型: Chain-of-Thought-Explanation（CoTE）
      - 基底模型：T5-Base (220M) 
      - 專為 DST 任務設計
      - 強調獲得槽位值所需的推理步驟，能在確定槽位值後逐步產出詳細的推理解釋，有助於生成更準確且可靠的槽位值
#### reference
- [DeepMind最新：发布说话者-推理者架构实现Agents快慢思考 | 融合系统1+系统2](https://blog.csdn.net/m0_59164520/article/details/143027392)

### 待分類
- [(2023)MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control](https://arxiv.org/abs/2310.18342)
- [(2024)Evolving to be Your Soulmate: Personalized Dialogue Agents with Dynamically Adapted Personas](https://arxiv.org/html/2406.13960v1)
- [(2024)Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations](https://arxiv.org/html/2405.17974v1)
- [(2025)In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents](https://arxiv.org/abs/2503.08026)
