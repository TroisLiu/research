# Dialogue System
## 依據對話目的分類
### 開放域對話（open-domain dialogue）
- 目標：
  - 需要展現出知識淵博、風趣且語言自然的特性
  - 追求對話的豐富性和連貫性，比如閒聊、問答、娛樂
  - 避免不當言論
  - 強調個性化和情感溝通等品質
- LLM：
  - 優點：經過大規模預訓練的模型本身掌握了廣泛的常識和知識，能夠就各種話題侃侃而談，並產生類似人類風格的回應
  - 缺點：容易天馬行空：例如對於未知領域的提問，模型可能不願承認自己的不知道，而編造看似合理但實則錯誤的答案（即幻覺問題）
  - 
### 任務導向對話（task-oriented dialogue, TOD）
- 目標：
  - 聚焦在完成特定目標上
  - 如: 預訂餐廳、查詢列車時刻
  - 需要與後端資料庫或API交互，遵循嚴格的業務邏輯
  - 要求對話精確且高效
- LLM：
  - 優點：讓LLM直接根據對話歷史生成對話行為和槽位填充結果，再據此產生回應，減少模組誤差傳遞
  - 缺點：容易天馬行空：例如對於未知領域的提問，模型可能不願承認自己的不知道，而編造看似合理但實則錯誤的答案（即幻覺問題）
- 作法
  - LLM在對話中自動決定何時調用API並填入參數，執行如資料庫查詢、計算等操作後再繼續對話
  - 混合式系統成為熱門方案：讓LLM處理自然語言理解和生成部分，但關鍵決策仍由顯式策略模組或規則把關
- 挑戰：
  - 當任務流程複雜或需要遵循業務政策時，僅靠在Prompt中添加大量指示往往不足以約束LLM的行為
  - LLM在任務型對話中的幻覺和不服從問題仍需關注
  - LLM在嚴格遵守對話流程上仍缺乏機制，開發者難以完全控制其每一步行為
### 融合風格對話
- 在主要任務流程中穿插寒暄和安撫，用戶既能得到任務結果也有良好體驗​
- 挑戰：
  - 如何動態調節兩種對話模式的切換、確保不互相干擾
- 
## 相關研究議題
### 對話狀態追蹤(Dialogue State Tracking, DST)
- 兩種設計方向
  - 分類式模型（classification-based
    - 傳統作法 
    - 從一組候選值中選取欄位的值   
    - 需事先定義對話本體（ontology），泛化能力較弱
    - 在受限領域中表現良好，但在更複雜、開放式的對話中，這些方法常難以泛化
  - 生成式模型（generation-based)
    - 直接產生欄位值，能處理未見過的新領域與新值 
- LLM較善於利用完整對話語境來糾正先前輪次可能出現的錯誤
- LLM-DST的整體性能隨對話輪數增加而下降得更慢，對錯誤傳播（error propagation）的抵抗力比傳統模型更佳
- 核心議題
  - 共指 Coreference
    - 指多個詞或短語在語境中指向同一個實體
    - Coreference Resolution是提升 DST 表現時的一大障礙：多輪對話中語言表達多樣性所導致，槽位與其值經常是以間接/代名詞方式表達
    - 對應議題
      - 代詞消解（Pronoun Resolution）: 處理「他、她、它、他們」等代詞指涉誰的問題
      - 命名實體共指（Named Entity Coreference）: 識別像「台達電子」與「Delta」、「台達電子工業股份有限公司」是否指同一個組織
      - 跨句共指（Cross-sentence Coreference）: 不同句子中指涉同一實體的辨識
      - 事件共指（Event Coreference）: 識別不同語句是否指的是同一個事件
      - 共指鏈（Coreference Chains）: 將所有指涉同一實體的詞彙串成一條鏈，供後續理解與推理使用
  - 錯誤傳播 Error Propagation
  - ![image](https://github.com/user-attachments/assets/9c2479a1-0860-4820-964f-f9360f72c129)

- 用對話狀態（如槽位-值集合）作為對話歷史的高度摘要
- 論文：
  - [(2022)"Do you follow me?": A Survey of Recent Approaches in Dialogue State Tracking](https://arxiv.org/abs/2207.14627)
  - [(2023)Towards LLM-driven Dialogue State Tracking](https://aclanthology.org/2023.emnlp-main.48.pdf)
    - 要解決的問題
      - 期望不依賴大型閉源模型(如 ChatGPT)，實現具強泛化能力、且可實際部署的 DST系統
        - ChatGPT 雖強，但不適用於真實應用情境:
          - 雖然 ChatGPT 在 DST 任務中表現優異，但其「閉源、無法本地部署、API 限制、資料隱私風險」等缺陷，使其難以應用於實際系統
          - ChatGPT 回答常附帶冗長解釋，影響準確性
        - 既有微調方法對 prompt 設計過於敏感: 傳統指令微調（instruction tuning）若使用固定模板，模型對 prompt 的格式極度敏感，導致泛化能力弱
        - 現有 Dialogue State Tracking (DST) 模型難以泛化至新領域（Zero-shot）: 多數傳統 DST 模型依賴大量標註資料，對未知領域（unseen domains）的適應力差
        - 大型模型微調成本高、資源需求大
        - LoRA 等 PEFT 方法仍需調參與結構設計考量
    - 貢獻:
      - 以GPT-3.5做實驗(擷取關鍵資訊)發現：
        - Prompt中若提供範例，對話範例會影響GPT取得關鍵資訊的正確率
        - 同時問多個Slot也會影響準確率
      - 提出LDST框架: 基於開源LLaMA模型驅動DST，包含"組合式領域-槽位指令微調(assembled domainslot instruction tuning method)"和"參數高效微調技術(parameter efficient tuning technique)"實現
    - 構建指令資料集
      - 以指令微調手法(針對任務設計的明確且具體的指令)來引導模型
      - 指令資料集
        - 以組合式領域-槽位指令生成法（Assembled Domain-Slot Instruction Generation)建構：透過隨機組合不同的指令模板與輸入模板來產生多樣化的指令樣本，讓模型在微調過程中接觸到各種形式的指令，有助於降低其對單一提示樣式的敏感度，提升泛化能力
        - 每一筆資料包含三個欄位：
          -  Instruction（任務指令）：會隨機二選一建構指令
            - 標準槽位追蹤指令（Standard Slot Tracking Instruction）
            - 自訂槽位追蹤指令（Customized Slot Tracking Instruction）：包含更具體的領域-槽位資訊
          -  Input（任務輸入）：由以下4部分組成
            - 對話上下文（dialogue context）：包含特殊區段Token: [USER], [SYSTEM]
            - 領域-槽位描述提示（domain-slot description prompt）：包含特殊區段Token: [domain], [slot]
            - 可能值列表（PVL, Possible Value List）提示：僅用於分類類型的槽位（categorical slots）
            - 查詢提示（query prompt）
          -  Output（期望輸出）
          -  提示詞設計是基於人工經驗的主觀選擇
        - 生成資料時，各有50%的機率生成"欠缺slot描述"/"欠缺可能值列表"的Input prompt
        - ![image](https://github.com/user-attachments/assets/b2355864-ce0e-4919-827f-271fa6407487)
      - 以PEFT方法微調模型
        - 以LoRA作微調
        - 基底模型: LLaMa 7B
        - 可學習參數：8.4M (總參數量的0.12%)
    - 資料集
      - ![image](https://github.com/user-attachments/assets/fd301ff8-22fc-46c8-95a8-3595961838f9)
      - Schema-Guided Dialogue(SGD)
        - 2020推出
        - 涵蓋16個領域、26個服務"
      - MultiWOZ 2.2 與 MultiWOZ 2.4
        - 2.2是2020推出；2.4是2022推出
        - 目的是提升 DST 任務的評估準確性
        - 2.4 版本的驗證集與測試集經過了精確的重新標註
    - 評估指標
      - Joint Goal Accuracy（JGA）
        - 模型在每一輪對話中**是否能完整正確地預測整個對話狀態（所有槽位的預測值）的比例**
        - DST 任務中的主要評估指標
        - 只有當一輪中的所有槽位都預測正確時，該輪才會被計入準確率
      - Average Goal Accuracy（AGA）
        - AGA 是每輪中**所有「啟用槽位（active slots）」**的平均準確率。
        - 啟用槽位: 指在當前輪對話中被提及，不是從前一輪繼承而來的槽位
    - 表現
      - ![image](https://github.com/user-attachments/assets/886bd8f2-6b1e-48bd-bd7d-a04cc6812eb4)
      - ![image](https://github.com/user-attachments/assets/94831723-f0dd-4034-a7fc-a86a2ee7de02)
      - ![image](https://github.com/user-attachments/assets/120e5116-720d-4604-92b4-8f77d30c3ff6)
      - ![image](https://github.com/user-attachments/assets/d5a6a37a-8f5d-4050-9e95-15c37f627621)
 
      - Error Propogation議題
        - ![image](https://github.com/user-attachments/assets/d8278b4d-52c6-4573-8354-00acf769d811)
        - LDST 的下降速度遠低於 LLaMa 與最佳基線方法，顯示其對錯誤傳遞的抵抗能力更強
    - 其他議題：
      - 當對話或描述內容過長時，如何有效截斷或摘要化輸入內容亦成為一項挑戰   
  - [(2024)Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation](https://arxiv.org/abs/2405.13037)
    - 要解決的問題
      - 如何在缺乏大量真實標註對話資料的情況下，有效訓練出適應多領域、可泛化的對話狀態追蹤（DST）模型
        - 對話狀態追蹤（DST）所需的標註資料成本高昂
        - 真實情境資料有限，缺乏足夠的資料會影響 DST 模型效能
        - 現有模型難以快速適應新領域: 多數 DST 模型依賴固定本體（ontology），泛化能力有限，無法即時拓展至未見過的任務或場景
        - 模擬對話生成的品質與一致性難以保證
        - 混合真實資料與生成資料時，可能出現資料分布不一致問題
    - slot分兩種
      - 類別型（categorical）：有一組候選值（例 <hotel-parking> = “True”）
      - 非類別型（non-categorical）：值為對話中的一段文本 （例 <hotel-name> = “Alexander”）
      - 若對話中未提及特定欄位，則該欄位的值設為 “NONE”
    - 貢獻
      - 提出LLM 支援的使用者–代理人模擬LUAS : 使用GPT-4模擬user和agent互動，生成帶有DST標籤的大量模擬對話，降低對話資料的收集與標註成本
      - 生成了近 8,000 筆新對話資料，包含
        - User&Agent對話
        - 對話意圖(雙方可控制意圖)
        - 對應Slot
      - 確保生成資料正確性並維持多樣性
        - 透過結構化Prompt控制生成的對話，避免生成的話題有偏差
        - 加入Slot Extraction檢查生成的對話內容Slot與Valut是否一致，避免生成的資料存在Error Propogation情況
        - 實作改寫模板擴增語言多樣性，避免過度重複
      - 然後用這些合成數據對LLaMA 2模型進行兩階段微調，在兩個資料集上，加入生成資料可明顯提升模型效能
      - 提出兩階段微調，避免生成資料與真實資料混和一起訓練模型導致資料分布不一致，避免訓練偏移 

    - LUAS
      - 首先，LLM 會生成一個使用者設定檔，描述該使用者對各項任務的偏好(包含如預算、距離等具體條件
      - LLM 被提示去模擬使用者與代理人之間的對話：
        - 使用者模擬器
          - 對話初始：提出需求並尋求推薦或協助預訂與購買
          - 後續對話：評估需求是否被滿足，並決定是否繼續對話
        - 代理人
          - 理解使用者需求、提供建議並採取適當行動
      - 透過這樣反覆的對話模擬，加上由 LLM 所驅動的欄位提取器，得以產出大量標註過的多輪對話資料
      - 將使用者與代理人的共通意圖進行抽象，並針對每種意圖設計專門提示語
        - 使用者意圖: Inform Requirement, Update Requirement, Ask for Recommendation, Inquire Properties, Ask for Action, General Chat
        - 代理人意圖: Inquire, Report Search Results, Recommendation, Answer, Report Action Result, General Chat
      - 模擬器也會被提示生成控制識別碼（control identifiers），標記該回應所對應的意圖。根據輸入的控制識別碼，模擬器需選擇適當的意圖並生成相應的回覆
      - 使用一個**欄位追蹤模組（slot tracking module）**來記錄哪些欄位已被填寫、哪些仍待補全
      - 使用一個基於 GPT-4 的欄位提取模型，以驗證生成的對話與欄位填寫結果是否一致，若發現不一致，該段對話必須重新生成，以維持語義與流程的一致性
      - 多樣性
        - 為確保生成資料具備豐富的語言變化，我們手動設計了 10 組改寫模板
        - 交由 GPT-4 擴充為數百組變化形式，用於提升使用者與代理人回應的語言多樣性
      - ![image](https://github.com/user-attachments/assets/14c8e24b-9b08-445f-95b8-3628f6b4d52a)
      - ![image](https://github.com/user-attachments/assets/f7addc90-c860-4b6d-a9f6-c77466f62fca)

    - 模型
      - 資料生成：gpt-4-1106-preview
      - 基底模型：LLaMa 2 7B
      - 基準模型：LUAS-R，以真實資料對 LLaMA 2 進行微調
      - 目標模型：LUAS-R+G，將生成資料與真實資料一同透過兩階段微調手法來微調LLaMA 2
      - 算力：8 張 Nvidia A100（80GB）GPU，並透過 PyTorch 的 FSDP 框架（Zhao et al., 2023）進行全參數監督式微調(每張 GPU 的 batch size 為 8)
      - 微調學習率：2e-5
      - 採用 Adam 優化器（Kingma and Ba, 2015），設置參數為 β1 = 0.9，β2 = 0.999，warm-up 比例設為 3%。每個微調階段約持續兩小時。推論階段則使用 vLLM（Kwon et al., 2023）執行
    - 微調方法：兩階段微調
      - 第一階段：使用生成的對話資料對 LLaMA 2 模型進行微調，學習基本的任務導向型對話模式
      - 第二階段：以真實對話資料繼續微調模型，協助模型對齊真實語料的語言分布，提升其在實際應用中的效能
    - ![image](https://github.com/user-attachments/assets/b69bfb33-99cc-472d-addc-071be3cce969)
    - 替代實驗(用生成資料替代真實資料)
      - 在 MultiWOZ 2.2 上針對不同領域進行資料替換實驗
      - 特定領域的所有對話片段移除，並將新生成的資料插入至被移除的位置
      - 替換後的新訓練資料集中，將包含 1 個使用生成資料的領域與其他 4 個使用真實資料的領域
      - 即使部分資料以生成內容替代，性能下降幅度相對訓練資料的減少而言非常輕微，表示生成資料在新領域中能有效協助模型適應並維持良好預測準確率
      - 生成資料能有效緩解資料不足所帶來的限制
    - 缺點
      - 生成資料品質的客觀驗證不足
        - 沒有語言流暢性評估(BLEUM ROUGE, BERTScore)
        - 沒有人工評估生成資料流暢性、合理性、任務完成度
        - 沒有人工評估比較生成資料與真實資料差異&相似度
        - 宣稱的自我檢查機制(Slot Consistency)只透過Slot Extraction來驗證檢查使用者提供的 utterance 與填入的 slot values 是否一致，但這只是內部一致性驗證，沒有進行外部正確性評估
      - 模擬資料雖高品質，但仍非真實對話語料，對話多樣性是個問題
        - 改寫是否真的涵蓋語義變異與語用多元性，未有實驗數據或對比說明
        - 缺乏與其他資料增強策略（如 paraphrasing, back-translation）之比較
      - 跨領域遷移的挑戰處理有限
        - 論文主要測試在 MultiWOZ 內部領域（如餐廳、旅館）中做資料替換，並未測試在外部新領域（如法律、醫療）上的泛化能力。
        - 若任務 ontology 差異太大，GPT-4 的模板模擬仍可能無法涵蓋新領域的實際語境需求。
      - 未深入分析錯誤案例
        - 不清楚哪些Slot類型最易出錯
        - 是因為llm幻覺? entity overlap ? 還是多輪推理導致?
  - [(2024)Large Language Models as Zero-shot Dialogue State Tracker through Function Calling](https://arxiv.org/abs/2402.10466)
    - 要解決的問題
      - **DST 所需的標註資料成本高昂**
      - **現有模型難以實現 Zero-shot 效能**: 有研究試圖以少量或跨領域資料進行遷移學習，但對於「未見領域」的泛化能力仍不理想
      - **ChatGPT 類模型的 DST 表現有限**: 在處理 DST 任務時，需特殊提示與格式規定，導致實務應用困難
      - **現有 Prompting 方法無法整合 DST 與回應生成**: 多數方法將 DST 視為獨立任務處理，無法自然整合於 LLM 的聊天流程中，不符合真實 TOD 系統需求
      - **開源模型難以達到商業模型的效能**: 中等規模開源模型（如 LLaMA、Vicuna）在缺乏函數呼叫能力與 DST 訓練的情況下，無法與 GPT-4 等進階模型競爭
    - 貢獻
      - 提出 FNCTOD：以函數呼叫實現零樣本對話狀態追蹤（Zero-shot DST）
        - 將 DST 任務視為「函數呼叫」問題
        - 每個任務導向領域（如餐廳、旅館）被建模為一個函數，對話狀態（slot-value pairs）作為該函數的參數
        - 將函數定義（function specification）嵌入系統提示（system prompt），讓聊天型 LLM 能在回應中自然產出函數呼叫與對話回應
        - 使用少量異質資料微調 LLaMA2，達成與 ChatGPT 相當表現
          - 使用來自 36 個領域、7,200 筆對話資料（非 MultiWOZ）對 LLaMA2-13B 進行微調
          - 微調後的模型 FNCTOD-LLaMA2-13B 可產生函數呼叫與自然語言回應，其 DST 效能與 ChatGPT 相當，具備實用價值
          - 讓 7B–13B 模型超越過往 ChatGPT 達成的 SOTA，且 GPT-4 表現提升達 14%
      - 提出兩階段函數呼叫生成流程，避免每輪對話都需重複載入完整函數規格，提升效能與成本效率
        - 階段一：函數選擇
        - 階段二：參數生成
        - 減少 token 數量（API 成本）同時提升預測準確率
      - 在多個中等規模&**未經微調**開源模型上達成或超越現有 SOTA
        - 在 未經微調的 7B/13B 開源模型（如 Zephyr、Baichuan、LLaMA2）上，透過情境提示（in-context prompting）即可超越先前只能用 GPT-4/Codex 實現的 SOTA 表現
        - 尤其 GPT-4 在本方法下比原先 SOTA 提示方法提升 5.6% 的 JGA
      - ![image](https://github.com/user-attachments/assets/01b7e0a1-1bf9-4159-95c3-2a8b3ad5c6f1)
      - ![image](https://github.com/user-attachments/assets/8cf17d1f-829a-4ad2-8a81-c3f74f49c22f)


    - 缺點
      - 現階段的 DST 準確率尚不足以實際部署
        - 雖然 FNCTOD 在多個模型上達到新 SOTA，但實驗中仍指出目前的準確度仍不足以應用在實際商業系統中
        - 在多輪長對話或複雜意圖中，仍可能出現錯誤的函數呼叫或漏判slot-value
      - 評估TOD 回應生成品質的方法不夠全面，無法對應真實情境
        - TOD 中的回應品質僅使用 去詞彙化（delexicalized）回應評估法，這種方法無法全面反映 LLM 回應的自然性與可用性
        - 此評估方式容易被「格式正確但語意不自然」的回應作弊，與 LLM 訓練的真實分布不符
      - 函數名稱錯誤可能導致連鎖錯誤
        - 雖有提出「函數選擇 + 參數生成」的兩階段策略，但若第一階段選錯函數，會影響後續生成參數的準確性
        - 「階段性錯誤累積」的情況會影響最終 DST 成效，特別在開源模型上更明顯
      - Zero-shot 能力仍需仰賴 prompt 設計或微調
        - 對於未經微調的開源模型，若沒有提供足夠的 in-context 範例，效果會明顯下降
        - 雖然聲稱 zero-shot，但實務上仍需謹慎 prompt 設計與函數格式調整，否則效果不穩定
      - 缺乏對多輪推理任務的處理能力討論
        - 雖整合函數呼叫與 DST，但未進一步探討是否能擴展至更高階任務（如多輪規劃、推理、目標分解）
  - [(2025) Interpretable and Robust Dialogue State Tracking via Natural Language Summarization with LLMs](https://arxiv.org/abs/2503.08857)
    - 要解決的問題
      - **結構化輸出(slot-value)缺乏對複雜語境與使用者意圖的表達力**
        - **難以捕捉多輪對話中的語境轉換與模糊、隱性的使用者需求**
        - 高度依賴人工設計規則與本體維護成本高
        - 結構化 DST 輸出格式可解釋性差
        - 多數 DST 方法無法應對使用者輸入中常見的錯字、語病或異常用語
    - 貢獻
      - 提出了 自然語言 DST（NL-DST） 框架，訓練 LLM 直接產出可讀性高的"人類語言狀態描述"取代Slot-Value
      - 微調模型的學習策略
        - Teacher Forcing: （讓模型看前一句來預測下一句）
        - Beam Search: （保留最可能的一群詞序列）
        - Nucleus Sampling: （只從機率前幾名的詞中選字）
    - Baseline模型
      - Rule-based Slot-Filling DST
      - Bert-based DST
      - GPT-2 DST
    - 評估指標
      - JGA
      - Slot Accuracy
      - Relevance 相關性 (人工評估)
      - Informativeness 資訊性 (人工評估)
    - 缺點
      - 沒揭露用來訓練模型學會任務(生成具高可讀性的"人類語言狀態描述")的資料集具體內容
        - 只提到資料集內的"人類語言狀態描述"是由語言專家人工標註，確保其品質與準確性
      - 沒揭露所謂的高可讀性的"人類語言狀態描述"須包含哪些內容
        - 自然語言輸出容易冗長、重複、偏離重點，但論文未討論：
          - 如何控制生成摘要的長度
          - 如何確保輸出資訊的精確性與聚焦度 
      - 沒揭露基底模型 
### RL與對話決策
- 類別
  - RLHF
    - [(2019)Reinforcement Learning for Personalized Dialogue Management](https://arxiv.org/abs/1908.00286)
      - 可能太舊 
    - [(2024)Knowledge acquisition for dialogue agents using reinforcement learning on graph representationsKnowledge acquisition for dialogue agents using reinforcement learning on graph representations](https://arxiv.org/html/2406.19500v1)
  - RLAIF 
- 論文：
  - [(2024)Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents](https://openreview.net/pdf?id=MCNqgUFTHI)
    - 讓一個可調參的小型策略模型作為插件指導LLM的對話決策
    - 以有標註數據進行監督微調，隨後採用自我對弈(self-play)方式讓LLM與自身模擬對話，透過AI代理提供的目標導向反饋進行強化學習​
    - 經過RL微調的策略插件可以顯著提升LLM在主動式對話任務，使LLM在沒有人工介入的情況下，不斷改進對話策略，並能快速遷移到新場景
### 外部知識增強對話管理
- 透過RAG改善LLM幻覺議題
- 這在任務型問答、推薦對話系統當中會很有幫助
- 系統
  - stanford的KITA
  - ChatCRS：工具增強的知識檢索代理、目標規劃代理
- 論文
  - [(2024)Knowledge-enhanced Response Generation in Dialogue Systems: Current Advancements and Emerging Horizons](https://aclanthology.org/2024.lrec-tutorials.13.pdf) 
### 上下文管理與長程對話
- LLM受限於有限的上下文視窗，在特別長的對話仍面臨挑戰
- 多數模型在輸入超過數千Token後，性能會下降，難以掌握對話中跨越很多倫次的因果關係、時間脈絡
- 資料集
  - LoCoMo長程對話資料集
    - 每段對話300輪次，9K個詞以上，有數十次獨立對話
    - 用來評估LLM長程記憶能力
- 對應策略
  - 模型架構調整 (1篇論文)
    - 增加模型上下文窗口：但可能丟失對話中間內容
    - 強化注意力機制
    - 優化 KV 快取
    - 改進位置編碼
  - 動態檢索相關歷史
    - 參考RAG概念，將超出上下文窗口的歷史對話內容存入向量化資料庫
    - 產生回應時，透過檢索相關的過往語句，將相關內容附加給LLM)
  - 摘要與壓縮歷史 (1篇論文)
    - 隨著對話進行，不斷壓縮舊對話內容生成摘要
    - 提取出對話狀態，以壓縮表示方式融入上下文
  - LLM 結合記憶模組：顯式對話記錄與狀態跟蹤 (3篇論文)
    - 構建一個外部記憶來存儲對話中的關鍵資訊或狀態，舉例
     - 之前已確認的事實
     - 用戶偏好
    - 每次需要產生回應時，將使用者輸入及相關資訊提供給模型
- 方法:
  - [(2023)MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://arxiv.org/pdf/2305.10250)
    - 屬於**LLM 結合記憶模組**
    - 要解決的問題
    - 貢獻
    - 缺點 
  - [(2024)Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/pdf/2406.05925)
    - 屬於**LLM 結合記憶模組**
    - 要解決的問題
      - 
    - 貢獻
    - 缺點
  - [(2024)THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation](https://arxiv.org/pdf/2406.10996v1)
    - 屬於**LLM 結合記憶模組**
    - 要解決的問題
      - 
    - 貢獻
    - 缺點
  - [(2024)Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement](https://arxiv.org/abs/2401.14215)
    - 要解決的問題
      - 如何在不損失資訊的前提下，有效處理與整合多輪對話中可能互相矛盾的人設句子，並提升長期對話中的回應品質與人設一致性 
        - 人工人設資訊過於貧乏，限制生成效果: 現有對話資料集中，多數由人類撰寫的人設句子太過通用或簡化，缺乏細節，導致生成的回應缺乏多樣性與吸引力
        - 基於常識的人設擴展容易導致人設矛盾: 即使透過 COMET 等常識模型對人設擴展，仍可能引入語義矛盾（如：「我很懶」與「我每天打掃房間」），尤其在多輪對話累積後更明顯
        - 現有方法處理矛盾人設的方式不夠理想
          - 一些方法會直接移除矛盾人設（如使用 NLI 模型過濾），但這樣會損失有用資訊。
          - 其他方法則會避免生成矛盾人設，但這與人類個性多樣且具情境依賴性的特性不一致
        - 缺乏針對多輪對話的人設精煉機制
          - 過去研究大多聚焦於單輪或短期對話，缺乏跨多輪對話下的人設擴展與整合機制
          - 長期對話中的人設需要根據不同語境進行情境感知的精煉 
    - 優點
      - 首次在多輪對話中探討基於常識的人設擴展（persona expansion）
        - 過去人設擴展大多集中在單輪對話或短期任務，這篇是首度聚焦於跨多輪（multi-session）對話場景 
        - 強調長期記憶中人設的管理與進化需求，對持續性對話的實用性高
      - 提出 CAFFEINE：情境感知的人設精煉框架
        - 所創框架能保留並轉化矛盾人設為具豐富資訊的敘述，而不是簡單移除或忽略
        - 透過三種策略（整合、釐清、保留）與 LLM 結合，讓矛盾的人設更貼近人類語言與思維邏輯
      - 設計圖結構的「逐步精煉（iterative refinement）」流程
        - 引入圖結構來追蹤與選取待精煉的矛盾人設對，提升效率與擴展性。
        - 精煉後的結果儲存進長期記憶，並動態更新圖結構
      - 結合 LLM（ChatGPT）執行常識推理與精煉判斷
        - 不只是生成，還讓 LLM 根據語境自選合適策略、說明理由，進一步產出經「人類化」調整後的人設句
      - 兼具時間與成本效益
    - 缺點
      - 過度依賴 COMET 等常識模型，品質有限
        - 所使用的 COMET 常識生成模型本身有錯誤率與偏差，若其產生的擴展內容品質不佳，將直接影響後續人設精煉與回應生成。
        - 作者也承認未來希望改用 LLM 本身進行人設擴展，但目前仍依賴外部模型，形成效能瓶頸。
      - NLI 模型有誤判風險，影響精煉圖構建
        - CAFFEINE 所依賴的矛盾判斷邏輯（δ）是基於 NLI 模型（RoBERTa-MNLI），但 NLI 模型無法考慮語境與細節語意
        - 可能出現：
          - 原本合理的人設被誤判為矛盾 → 不必要地精煉甚至錯誤合併
          - 真正需要處理的人設未被檢出 → 被遺漏處理
      - 僅處理單一使用者人設，缺乏對雙方人設互動的建模
        - 精煉流程是逐一針對單一說話者的人設處理，而非同時考量「雙方在同一主題上可能展現不同特質」的互動動態
      - 未充分處理 LLM 輸入長度限制問題
      - 人設精煉與回應生成的區隔不夠明確
        - 雖然兩者功能不同（精煉 vs. 回應），但實驗與架構整合度高，讀者可能會混淆兩階段的貢獻來源。
        - 缺乏單獨 ablation study 分析各個策略（如 Resolution vs. Disambiguation）在不同任務中的效果差異。

  - [(2024)StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses](https://openreview.net/pdf?id=eNvVjpx97O)
    - 要解決的問題
      - LLM在處理長對話時效率低落
        -  LLaMA2為例[6]，當輸入超過 4,096 的上下文長度時，其推論能力會明顯下降。
        -  此外，注意力機制 [7] 的計算複雜度隨文字長度呈二次成長，導致 GPU 記憶體使用量增加、 
      - 長對話中歷史資訊逐漸流失
      - 稀疏注意力方法雖然加快處理速度，但記憶保留能力不足
      - 缺乏高效的壓縮對話歷史機制
      - 現有模型難以支持「終身對話學習」或極長序列建模
      - 尚無策略能有效學習「對話注意力匯聚點」的資訊聚合能力
    - 貢獻
      - 發現對話中特殊的「發話結束」標記（End-of-Utterance）EoU tokens 的資訊聚合潛力，並提出 conv-attn sinks 概念
        - 利用此作為會話注意匯聚點 
        - 在多輪對話中，用來分隔發言的特殊 token（如 </s> 或換行符 \n）在注意力分布上具有顯著聚合特性
        - 「對話注意力匯聚點（conversational attention sinks, conv-attn sinks）」 
      - 提出 StreamingDialogue 架構，支援長對話生成
        - 設計一個能在保留關鍵資訊的情況下，大幅減少計算與記憶體成本的對話框架
          - 4 倍速度提升
          - 18 倍記憶體使用量降低 
        - 設計兩項學習策略，強化模型的資訊聚合與記憶能力，減少壓縮帶來的信息損失​
          - SMR（短期記憶重建）：引導模型學會將發言資訊聚合到 conv-attn sink(讓模型僅能透過目標發言的 conv-attn sink 來完成重建)，促使該 sink 有能力從目標句子中恢復資訊，並以其重建內容
          - LMR（長期記憶重啟）：訓練模型將最終發言視為查詢，從歷史對話中召回特定回應(僅針對對話歷史中的 conv-attn sinks 進行注意力操作)，加強對話上下文的長期引用能力
        - 可在未經訓練的 LLM（如 LLaMA 2/3、Mistral）中應用，仍具資訊保留能力
          - 架構外部化（externalized design）
          - 僅修改輸入的 Attention Mask，不改模型參數
            - 關鍵改動在於如何設定 Attention Mask，引導模型僅對 conv-attn sinks、第一個 token、以及最近兩輪的 token 進行注意力運算
          - 在推論階段只快取關鍵位置（conv-attn sinks）以節省記憶體與計算
            - 和 FlashAttention 類似，StreamingDialogue 能在推論階段套用自身的記憶管理與注意力範圍限制，僅保留 conv-attn sinks 的 KV-cache
            - 即使是如 LLaMA-2/3、Mistral 等原生不支援長記憶的模型，只要支援 custom attention mask / KV cache pruning，就可以直接套用 
        - 長度外推穩定性佳，對話長達 25K token 仍能維持品質
          - 能準確回憶長達 44 輪前的發言內容，展現卓越的長期記憶能力 
    - StreamingDialogue
      - 僅需快取
        - 首個 token
        - conv-attn sinks
        - 最近兩輪發言的 tokens
    - 資料集
      - PersonaChat [35]
      - Multi-Session Chat（MSC）[36]: 延伸的對話上下文
      - Topical-Chat [37]
      - MultiWOZ [38]  
    - Baseline方法
      - Dense Attention（密集注意力）[7]：用來捕捉所有資訊
      - Local Attention [14]：具有固定視窗限制
      - Big Bird [39]：結合滑動視窗、全域與隨機注意力
      - StreamingLLM [15]：在固定視窗內額外關注注意力匯聚點
      - MemBART [40]：帶記憶模組的 Transformer 編碼解碼模型
      - HRED [41]：具階層式編碼器，可編碼任意長度的對話
      - VHRED [42]：具階層式編碼器，可編碼任意長度的對話
    - 評估指標
      - BLEU（B-avg / B-1 / B-2） [43]：B-avg 為 BLEU-1 到 BLEU-4 的平均
      - ROUGE（R-1 / R-2 / R-L） [44]
      - Distinct（D-1 / D-2 / D-3） [45]
      - USL-H [46] 與 Dial-M [47]：兩項無參考基準的對話品質評估指標
      - Perplexity (PPL)
      - C score [48]（在 PersonaChat 中衡量對話一致性）
      - 人工評估
    - 缺點
      - 缺乏對 “高可讀性重建文本” 的質化分析與語言品質驗證
        - 雖然提出 SMR（短期記憶重建）任務以驗證資訊聚合能力，但僅使用 BLEU-1（89.19%）來評估重建效果，未進一步分析生成文本是否具有「語義完整」、「上下文流暢」或「符合語用邏輯」等層面 
        - 缺少人類評估對重建語句可讀性、合理性、與原始語句一致性的分析 
      - 對 conv-attn sinks 的理論基礎與學理探討不足
        - 雖然發現分隔符如 </s> 有注意力聚合現象，但未深入語言學或以建模機制解釋為何這能聚合語義資訊
        - 未明確區分 attention 聚合 vs. semantic aggregation 的差異，可能混淆注意力重點與實際語義承載能力
      - 缺乏與其他「長期記憶結構」的綜合比較與結合討論
        - 僅與部分稀疏注意力與記憶增強方法（如 MemBART、StreamingLLM）比較，未納入 Retrieval-Augmented Generation (RAG)、External Memory Module、或 Memory Compression Transformer 等代表性架構進行分析與對照
      - 效能提升的可轉移性仍有待驗證
      - 推論效率雖提升，但上下文推理能力未深入探討
        - 論文重點在壓縮與資訊保留，但未呈現本方法對「跨多輪推理」、「因果關係延續」、「角色一致性維持」等高階語用任務的具體優勢
        - 尚無實驗設計能驗證 conv-attn sinks 是否有助於真正的上下文理解與長程邏輯維繫
  - [(2025)In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents](https://arxiv.org/abs/2503.08026)
    - 屬於"摘要與壓縮歷史"
    - 要解決的問題
      - 如何讓 LLM 在長期對話中能有效、動態地記住過去資訊，並根據當前對話任務，準確地擷取與整合相關記憶，生成具一致性與個人化的回應。
        - LLM 無狀態（stateless）特性，無法記住長期對話歷史 : 現有 LLM 雖可進行開放式對話，但缺乏記憶機制，難以維持跨 session 的一致性與個人化
        - 現有記憶管理方法使用固定粒度，導致語意片段化 : 多數系統依據「回合」或「階段」為邊界進行記憶切分，但這常常無法對應對話的語意結構，導致關鍵資訊分散，難以有效擷取。
        - 記憶擷取器靜態、難以適應不同對話情境與使用者偏好 : 現有擷取機制（retrievers）通常為固定策略，無法根據實際回應的需要動態優化擷取結果
    - 貢獻
      - 提出框架Reflective Memory Management(RMM)
        - 簡單說：讓 AI 能記得你說過的話，而且記得對的重點，用在對的時候
          - 你跟 AI 聊了一些東西（可能好幾次）
          - 它會整理你說過的主題（前瞻性反思）
          - 下次你提問，它去記憶庫找相關筆記
          - 回答後，它再反思自己是不是有用到正確的記憶（回顧性反思）
          - 記憶庫隨著時間會越整理越聰明
        - (1) 前瞻性反思（Prospective Reflection）:
          - 簡單說：聰明整理筆記
          - 動態地將互動內容依不同粒度（如發話、回合、對話階段）進行摘要，存入個人化記憶庫以利未來擷取
          - 持續維持對話歷史的一致性與整合性
          - 兩步驟
            - 記憶擷取（memory extraction）: 利用 LLM 從當前 session 中擷取對話片段，針對每個明確提及的主題，生成對應的主題摘要
            - 記憶更新（memory update）:
              - 對每一條新擷取的記憶，我們從記憶庫中找出語意上最相似的前 K 筆記憶項目
              - 使用另一個 LLM
                - 判斷該筆新記憶應該直接新增進記憶庫（例如：屬於新主題）?
                - 與現有的相似記憶合併為一條更新後的記憶（例如：針對已討論過的主題新增新資訊）?
        - (2) 回顧性反思（Retrospective Reflection）:
          - 簡單說：自我檢討找關鍵
          - 基於 LLM 所引用的證據，透過線上強化學習方式持續優化記憶擷取策略
            - 若 LLM 回應中引用該記憶 → +1（有用）
            - 若未引用該記憶 → −1（無用) : 如果沒用上，它會把這些記憶「降權重」，下次不再亂用
          - ![image](https://github.com/user-attachments/assets/34065f19-1118-481c-b610-75dc02b23b71) 
        - 組成
          - 記憶庫（Memory Bank）
            - 將對話歷史儲存為一系列記憶項目（memory entries），每項記憶為一組二元組 (主題摘要, 原始對話)。
            - 「主題摘要（topic summary）」作為檢索的關鍵字，用以對應到原始的對話片段。
          - 擷取器（Retriever）
            - 根據使用者當前的提問，識別出最相關的記憶項目
            - 密集向量擷取器（dense retrievers）
              - Contriever（facebook/contriever）（Izacard et al., 2022）：一種基於對比學習的語意搜尋專用擷取器。
              - Stella（dunzhang/stella_en_1.5B_v5）（Zhang et al., 2024b）：基於語言模型訓練的大型嵌入式擷取器。
              - GTE（Alibaba-NLP/gte-Qwen2-7B-instruct）（Li et al., 2023）：設計用於指令型查詢，涵蓋多語言、多領域語料進行訓練的擷取器。
            - 設定
              - 在未搭配重排序器（reranker）的情況下，Top-𝐾 預設為 5
              - 若搭配 reranker，則 Top-𝐾 為 20、Top-𝑀 為 5
            - 實驗觀察
              - 增加記憶數量（M）能穩定提升效能 
              - 使用 GTE 與 Stella 這類強大擷取器時，Top-𝐾/Top-𝑀 的調整帶來的效益最為明顯，顯示擷取品質本身是一個重要因素 
          - 重排序器（Reranker）
            - 對擷取器的初始輸出進行再排序，優先保留最關鍵的記憶內容
            - 讓系統能動態調整擷取策略
              - 向量適應（Embedding Adaptation）
              - 使用 Gumbel 技巧的隨機取樣（Stochastic Sampling with Gumbel Trick）
              - 並對每筆記憶分數加入 Gumbel 雜訊（Gumbel, 1954）以進行隨機取樣
              - 接著使用 softmax 將擾動後的分數正規化為機率分布
              - 基於有無引用記憶，產生回饋訊號
                - 若 LLM 回應中引用該記憶 → +1（有用）
                - 若未引用該記憶 → −1（無用）
              - 以REINFORCE演算法進行增強學習
            - 處理由擷取器取得的前 K 筆記憶向量（embeddings），並根據當前使用者提問的語意，挑選出前 M 筆最相關的記憶。
              - 增加記憶數量（M）能穩定提升效能 
          - 大型語言模型（LLM）
            - 將當前上下文與擷取到的相關記憶整合起來，生成個人化回應
            - 根據實際使用的記憶內容產生「回饋訊號」，這些訊號會用於回顧性反思階段，進一步精緻化重排序器
    - 資料集
      - MSC（Xu et al., 2022）：多階段對話基準資料集
      - LongMemEval（Wu et al., 2024）：長期記憶對話評估資料集
    - 使用模型
      - Gemini-1.5-Flash
    - 算力
      - 16 顆 NVIDIA A100 (CUDA 版本：12.2)
      - 40G 記憶體 
    - Baseline
      - No History
      - Long Context
      - RAG（檢索式生成）: 預設使用「回合」為擷取粒度，以獲得較佳表現
      - MemoryBank（Zhong et al., 2024）：將對話歷史視為靜態資料庫，並根據遺忘曲線原則進行擷取調節
      - LD-Agent（Li et al., 2024b）：使用固定資料庫，並額外搭配關鍵字比對等策略進行擷取調整       
    - 缺點
      - 依賴強化學習，成本高昂 (可能造成訓練成本高與收斂速度慢的問題)
      - 記憶更新流程潛在複雜且未量化成本
        - 動態擷取主題摘要、相似度比對、記憶合併 
      - 無針對錯誤擷取記憶對回應的影響做深入探討
      - 對 Citation-based Reward 的可信度與泛化性探討不足 (雖有驗證 citation precision/recall，但缺乏更深層的語用層級分析，例如 citation 是否代表因果貢獻、是否對生成決策構成實質影響)
### 以推理引擎增強對話管理
- 論文
  - [(2024)Chain of Thought Explanation for Dialogue State Tracking](https://arxiv.org/html/2403.04656v1)
    - 貢獻
      - 提出模型
        - Chain-of-Thought-Explanation, CoTE
        - CoTE-refined: CoTE + 自動同義改寫（paraphrasing）機制
      - 資料集
        - MultiWOZ 2.2
        - WoZ 2.0 (2017): 來自 Wizard-of-Oz 實驗的資料集，使用者透過打字輸入查詢（非語音輸入），因此對話上下文更為複雜。
        - M2M
          - 包含來自兩個領域（餐廳與電影）的 3,000 筆模擬對話，後續經人工校正。
          - 將餐廳與電影子資料集分別稱為 M2M-R 和 M2M-M，合併版則記為 M2M-R+M。 
    - 概念
      - 將CoT引入至DST當中，模型在決定槽位值後生成逐步推理的解釋
        - ![image](https://github.com/user-attachments/assets/03259e63-8067-473d-af85-db8f73b0c0b4)

        -  粗略解釋（Coarse Explanation)
        -  精緻解釋（Refined Explanation）
      - 能引導模型從相關對話輪中蒐集資訊並推理正確的槽值，從而提高預測的準確可靠性
    - 模型: Chain-of-Thought-Explanation（CoTE）
      - 基底模型：T5-Base (220M) 
      - 專為 DST 任務設計
      - 強調獲得槽位值所需的推理步驟，能在確定槽位值後逐步產出詳細的推理解釋，有助於生成更準確且可靠的槽位值
#### reference
- [DeepMind最新：发布说话者-推理者架构实现Agents快慢思考 | 融合系统1+系统2](https://blog.csdn.net/m0_59164520/article/details/143027392)

### 待分類
- [(2023)MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control](https://arxiv.org/abs/2310.18342)
- [(2024)Evolving to be Your Soulmate: Personalized Dialogue Agents with Dynamically Adapted Personas](https://arxiv.org/html/2406.13960v1)
- [(2024)Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations](https://arxiv.org/html/2405.17974v1)
- [(2025)In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents](https://arxiv.org/abs/2503.08026)
